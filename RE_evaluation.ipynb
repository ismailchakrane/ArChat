{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archat Evaluation\n",
    "\n",
    "## References Extraction\n",
    "\n",
    "To evaluate our approach, we manually recorded the number of references in the 10 articles and compared the OpenAI o1 model and the DeepSeek-R1 model in terms of accuracy (number of generated references / number of ground truth references in the articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent\n",
      "\n",
      "neural networks. In EMNLP, 2013.\n",
      "\n",
      "[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\n",
      "\n",
      "arXiv preprint arXiv:1409.0473, 2014.\n",
      "\n",
      "[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal of\n",
      "\n",
      "Machine Learning Research, pages 1137–1155, 2003.\n",
      "\n",
      "[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.\n",
      "\n",
      "IEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n",
      "\n",
      "[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase represen-\n",
      "tations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078,\n",
      "2014.\n",
      "\n",
      "[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\n",
      "\n",
      "In CVPR, 2012.\n",
      "\n",
      "[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\n",
      "vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special\n",
      "Issue on Deep Learning for Speech and Language Processing, 2012.\n",
      "\n",
      "[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network\n",
      "\n",
      "joint models for statistical machine translation. In ACL, 2014.\n",
      "\n",
      "[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based machine\n",
      "\n",
      "translation systems for wmt-14. In WMT, 2014.\n",
      "\n",
      "[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,\n",
      "\n",
      "2013.\n",
      "\n",
      "[11] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classiﬁcation: labelling\n",
      "\n",
      "unsegmented sequence data with recurrent neural networks. In ICML, 2006.\n",
      "\n",
      "[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In\n",
      "\n",
      "ICLR, 2014.\n",
      "\n",
      "[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,\n",
      "T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE\n",
      "Signal Processing Magazine, 2012.\n",
      "\n",
      "[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master’s thesis, Institut fur Infor-\n",
      "\n",
      "matik, Technische Universitat, Munchen, 1991.\n",
      "\n",
      "[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty\n",
      "\n",
      "of learning long-term dependencies, 2001.\n",
      "\n",
      "[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n",
      "[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.\n",
      "[18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013.\n",
      "[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural\n",
      "\n",
      "networks. In NIPS, 2012.\n",
      "\n",
      "[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building\n",
      "\n",
      "high-level features using large scale unsupervised learning. In ICML, 2012.\n",
      "\n",
      "[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\n",
      "\n",
      "Proceedings of the IEEE, 1998.\n",
      "\n",
      "[22] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\n",
      "\n",
      "Technology, 2012.\n",
      "\n",
      "[23] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based\n",
      "\n",
      "language model. In INTERSPEECH, pages 1045–1048, 2010.\n",
      "\n",
      "[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\n",
      "\n",
      "translation. In ACL, 2002.\n",
      "\n",
      "[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks. arXiv\n",
      "\n",
      "preprint arXiv:1211.5063, 2012.\n",
      "\n",
      "[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio. Overcoming the\n",
      "curse of sentence length for neural machine translation using automatic segmentation. arXiv preprint\n",
      "arXiv:1409.1257, 2014.\n",
      "\n",
      "[27] A. Razborov. On small depth threshold circuits.\n",
      "\n",
      "In Proc. 3rd Scandinavian Workshop on Algorithm\n",
      "\n",
      "Theory, 1992.\n",
      "\n",
      "[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.\n",
      "\n",
      "Nature, 323(6088):533–536, 1986.\n",
      "\n",
      "[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/˜schwenk/cslm_\n",
      "\n",
      "joint_paper/, 2014. [Online; accessed 03-September-2014].\n",
      "\n",
      "[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTER-\n",
      "\n",
      "SPEECH, 2010.\n",
      "\n",
      "[31] P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.\n",
      "\n",
      "9\n",
      "\n",
      " model output {\"references\": [\n",
      "{\"title\": \"M. Auli, M. Galley, C. Quirk, and G. Zweig.\", \"authors\": [\"M. Auli\", \"M. Galley\"], \"date\": \"2013\", \"journal\": \"EMNLP\"},\n",
      "{\"title\": \"D. Bahdanau, K. Cho, and Y. Bengio.\", \"authors\": [\"D. Bahdanau\", \"K. Cho\", \"Y. Bengio.\"], \"date\": \"2014\", \"journal\": \"arXiv preprint arXiv:1409.0473\"},\n",
      "{\"title\": \"Y. Bengie...\", \"authors\": [\"Y. Bengio\", \"R. Ducharme\", \"P. Vincent\", \"C. Jauvin.\"], \"date\": \"2003\", \"journal\": \"Journal of Machine Learning Research\"},\n",
      "{\"title\": \"Y. Bengio, P. Simard, and P. Frasconi.\", \"authors\": [\"Y. Bengio\", \"P. Simard\", \"P. Frasconi.\"], \"date\": \"1994\", \"journal\": \"IEEE Transactions on Neural Networks\"} ,\n",
      "{\"title\": \"K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio.\", \"authors\": [\"K. Cho\", \"B. Merrienboer\", \"C. Gulcehre\", \"F. Bougares\", \"H. Schwenk\", \"Y. Bengio.\"], \"date\": \"2014\", \"journal\": \"arXiv preprint arXiv:1409.1257\"} ,\n",
      "{\"title\": \"A. Razborov.\", \"authors\": [\"A. Razborov.\"], \"date\": \"1992\", \"journal\": \"Proc. 3rd Scandinavian Workshop on Algorithm Theory\"} ,\n",
      "{\"title\": \"D. Rumelhart, G. E. Hinton, and R. J. Williams.\", \"authors\": [\"D. Rumelhart\", \"G. E. Hinton\", \"R. J. Williams.\"], \"date\": \"1986\", \"journal\": \"Nature\"} , {\"title\": \"H. Schwenk.\", \"authors\": [\"H. Schwenk.\"], \"date\": \"2014\", \"journal\": \"arXiv preprint arXiv:1409.2935\"} ,\n",
      "{\"title\": \"M. Sundermeyer, R. Schluter, and H. Ney.\", \"authors\": [\"M. Sundermeyer\", \"R. Schluter\", \"H. Ney.\"], \"date\": \"2010\", \"journal\": \"INTER-SPEECH\"} , {\"title\": \"Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\", \"authors\": [\"Y. LeCun\", \"L. Bottou, L. Bottou, Y. Bengio, P. Haffner.\"], \"date\": \"1998\", \"journal\": \"Proceedings of the IEEE\"} , {\"title\": \"M. Vanderwalt, R. Schwenk, and J. K. P. S. W. van den Heuvel\", \"authors\": [\"M. Vanderwalt\", \"R. Schwenk\", \"J. K. P. S. van den Heuvel\"], \"date\": \"2013\", \"journal\": \"http://www Li um.univ-lemans.fr/~schwenk/cslm/...\"}\n",
      "]}\n",
      "titles_count 11\n",
      "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\n",
      "arXiv preprint\n",
      "\n",
      "ton. 2016. Layer normalization.\n",
      "arXiv:1607.06450 .\n",
      "\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\n",
      "gio. 2014. Neural machine translation by jointly\n",
      "arXiv preprint\n",
      "learning to align and translate.\n",
      "arXiv:1409.0473 .\n",
      "\n",
      "Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-\n",
      "cehre, Dzmitry Bahdanau, Fethi Bougares, Holger\n",
      "Learning\n",
      "Schwenk, and Yoshua Bengio. 2014.\n",
      "phrase representations using rnn encoder-decoder\n",
      "for statistical machine translation. arXiv preprint\n",
      "arXiv:1406.1078 .\n",
      "\n",
      "Jonas Gehring, Michael Auli, David Grangier, De-\n",
      "nis Yarats, and Yann N Dauphin. 2017. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint\n",
      "arXiv:1705.03122 .\n",
      "\n",
      "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\n",
      "Aaron van den Oord, Alex Graves, and Koray\n",
      "Kavukcuoglu. 2016. Neural machine translation in\n",
      "linear time. arXiv preprint arXiv:1610.10099 .\n",
      "\n",
      "Diederik Kingma and Jimmy Ba. 2014. Adam: A\n",
      "method for stochastic optimization. arXiv preprint\n",
      "arXiv:1412.6980 .\n",
      "\n",
      " model output {\"references\": [\n",
      "    { \"title\": \"Layer normalization\", \"authors\": [\"Jimmy Lei Ba\", \"Jamie Ryan Kiros\", \"geoffrey E Hin\"], \"date\": \"2016\", \"journal\": null },\n",
      "    { \"title\": \"Neural machine translation by jointly learning to align and translating. arXiv preprint\", \"authors\": [\"Dzmitry Bahdanau\", \"Kyunghyun Cho\", \"yoshua ben gio\"], \"date\": \"2014\", \"journal\": null },\n",
      "    { \"title\": \"Phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint\", \"authors\": [\"Kyunghyun Cho\", \"bart van merri¨enboeuer, caglar Gulcehre, dzmitry bahdanau\", \"fethi bougares, holger learning scwenk\", \"yoshua ben bogio\"], \"date\": \"2014\", \"journal\": null },\n",
      "    { \"title\": \"Convolu-tional sequence to sequence learning. arXiv preprint\", \"authors\": [\"Jonas Gehring\", \"Michael Auli\", \"David Grangier, De nitis Yarats, and yann N Dauphin\"], \"date\": \"2017\", \"journal\": null },\n",
      "    { \"title\": \"Neural machine translation in linear time. arXiv preprint\", \"authors\": [\"Diederik Kingma and Jimmy Ba\"], \"date\": \"2014\", \"journal\": null }\n",
      "] }\n",
      "    \n",
      "titles_count 5\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
      "\n",
      "transformers for language understanding. In NAACL-HLT, 2018.\n",
      "\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\n",
      "\n",
      "unsupervised multitask learners. 2019.\n",
      "\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n",
      "Luke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv,\n",
      "abs/1907.11692, 2019.\n",
      "\n",
      "Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv, abs/1907.10597, 2019.\n",
      "\n",
      "Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\n",
      "\n",
      "nlp. In ACL, 2019.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "\n",
      "and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n",
      "\n",
      "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\n",
      "Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\n",
      "processing, 2019.\n",
      "\n",
      "Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006.\n",
      "\n",
      "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv,\n",
      "\n",
      "abs/1503.02531, 2015.\n",
      "\n",
      "Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\n",
      "Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\n",
      "books. 2015 IEEE International Conference on Computer Vision (ICCV), pages 19–27, 2015.\n",
      "\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\n",
      "\n",
      "multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2018.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "\n",
      "Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.\n",
      "\n",
      "Alex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\n",
      "Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\n",
      "Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\n",
      "Patry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\n",
      "text understanding models. http://jiant.info/, 2019.\n",
      "\n",
      "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning\n",
      "\n",
      "word vectors for sentiment analysis. In ACL, 2011.\n",
      "\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\n",
      "\n",
      "comprehension of text. In EMNLP, 2016.\n",
      "\n",
      "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\n",
      "\n",
      "knowledge from bert into simple neural networks. ArXiv, abs/1903.12136, 2019.\n",
      "\n",
      "Debajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv, abs/1904.00796, 2019.\n",
      "\n",
      "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\n",
      "\n",
      "of student initialization on knowledge distillation. ArXiv, abs/1908.08962, 2019.\n",
      "\n",
      "Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\n",
      "\n",
      "distillation for web-scale question answering system. ArXiv, abs/1904.09636, 2019.\n",
      "\n",
      "Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practical\n",
      "\n",
      "bert models for sequence labeling. In EMNLP-IJCNLP, 2019.\n",
      "\n",
      "Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019.\n",
      "\n",
      "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited\n",
      "\n",
      "numerical precision. In ICML, 2015.\n",
      "\n",
      "5\n",
      "\n",
      " model output {\"references\": [\n",
      "    {\"title\": \"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.\",\n",
      "       \"authors\": [\"Jacob Devlin\", \"Ming-Wei Chang\", \"Kenton Lee\", \"Kristina Toutanova\"],\n",
      "       \"date\": \"2018\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.\",\n",
      "       \"authors\": [\"Alec Radford\", \"Jeffrey Wu\", \"Rewon Child\", \"David Luan\", \"Dario Amodei\", \"Ilya Sutskever\"],\n",
      "       \"date\": \"2019\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.\",\n",
      "       \"authors\": [\"Yinhan Liu\", \"Myle Ott\", \"Naman Goyal\", \"Jingfei Du\", \"Mandar S. Joshi\",\n",
      "       \"Danqi Chen\", \"Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov\"],\n",
      "       \"date\": \"2019\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv, abs/1907.10597, 2019.\",\n",
      "       \"authors\": [\"Roy Schwartz\", \"Jesse Dodge\", \"Noah A. Smith\", \"Oren Etzioni\"],\n",
      "       \"date\": \"2019\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In ACL, 2019.\",\n",
      "       \"authors\": [\"Emma Strubell\", \"Ananya Ganesh, and Andrew McCallum\"],\n",
      "       \"date\": \"2019\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\",\n",
      "       \"authors\": [\"Ashish Vaswani\", \"Noam Shazeer, Niki Parmar, Jakob Uszkoreit\",\n",
      "       \"Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin\"],\n",
      "       \"date\": \"2017\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Thomas P. M. Lee, Thomas B. D. K. F. C. H. C. C. C. C. C. C. C. C. C. C. C. C. C.\",\n",
      "       \"authors\": [\"Thomas P. M. Lee\",\n",
      "       \"Thomas B. D. K. F. C. C. C. C. C. C. C. C. C. C. C. C. C. C.\"],\n",
      "       \"date\": \"2020\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge distillation for web-scale question answering system. ArXiv, abs/1904.09636\",\n",
      "       \"authors\": [\"Ze Yang\", \"Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang\"],\n",
      "       \"date\": \"2019\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practicalbert models for sequence labeling.\",\n",
      "       \"authors\": [\"Henry Tsai\", \"Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer\"],\n",
      "       \"date\": \"2019\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Paul Michel, Omerlazy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019.\",\n",
      "       \"authors\": [\"Paul Michel, Omer Lazy, and Graham Neubig\"],\n",
      "       \"date\": \"2019\",\n",
      "       \"journal\": null},\n",
      "    {\"title\": \"Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In ICML, 2015.\",\n",
      "       \"authors\": [\"Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan\"],\n",
      "       \"date\": \"2015\",\n",
      "       \"journal\": null}\n",
      "    ]\n",
      "}\n",
      "titles_count 11\n",
      "Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolu-\n",
      "\n",
      "tional networks. CoRR, abs/1612.08083, 2016. URL http://arxiv.org/abs/1612.08083.\n",
      "\n",
      "Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In Proceedings\n",
      "of the fourteenth international conference on artiﬁcial intelligence and statistics, pages 315–323, 2011.\n",
      "\n",
      "Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error\n",
      "\n",
      "linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606.08415.\n",
      "\n",
      "Andriy Mnih and Geoﬀrey Hinton. Three new graphical models for statistical language modelling.\n",
      "\n",
      "In\n",
      "\n",
      "Proceedings of the 24th international conference on Machine learning, pages 641–648, 2007.\n",
      "\n",
      "Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\n",
      "arXiv e-prints, 2019.\n",
      "\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\n",
      "\n",
      "machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n",
      "\n",
      "Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint\n",
      "\n",
      "arXiv:1710.05941, 2017.\n",
      "\n",
      "Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. arXiv\n",
      "\n",
      "preprint arXiv:1804.04235, 2018.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "\n",
      "and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n",
      "\n",
      "4\n",
      "\n",
      "\fAlex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE:\n",
      "arXiv preprint\n",
      "\n",
      "A multi-task benchmark and analysis platform for natural language understanding.\n",
      "arXiv:1804.07461, 2018.\n",
      "\n",
      "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\n",
      "and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding\n",
      "systems. arXiv preprint arXiv:1905.00537, 2019.\n",
      "\n",
      "5\n",
      "\n",
      " model output { \"references\": [ { \"title\": \"Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier.\", \"authors\": [\"Angela Fan\", \"Michael Auli\", \"David Grangier\"], \"date\": \"2016\", \"journal\": \"CoRR\" } ] }\n",
      "    \t \t  \t \t  \t \t  \t\n",
      "titles_count 1\n",
      "[Alvarez and Salzmann, 2017] Jose M Alvarez and Mathieu\n",
      "Salzmann. Compression-aware training of deep networks.\n",
      "In NIPS, 2017.\n",
      "\n",
      "[Avron et al., 2012] Haim Avron, Satyen Kale, Shiva Prasad\n",
      "Kasiviswanathan, and Vikas Sindhwani. Efﬁcient and\n",
      "practical stochastic subgradient descent for nuclear norm\n",
      "regularization. In ICML, 2012.\n",
      "\n",
      "[Chen et al., 2015] Wenlin Chen, James Wilson, Stephen\n",
      "Tyree, Kilian Weinberger, and Yixin Chen. Compressing\n",
      "neural networks with the hashing trick. In ICML, 2015.\n",
      "[Courbariaux and Bengio, 2016] Matthieu Courbariaux and\n",
      "Yoshua Bengio. Binarynet: Training deep neural networks\n",
      "with weights and activations constrained to +1 or -1. arXiv\n",
      "preprint arXiv:1602.02830, 2016.\n",
      "\n",
      "[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\n",
      "Imagenet: A large-scale\n",
      "\n",
      "Jia Li, Kai Li, and Li Fei-Fei.\n",
      "hierarchical image database. CVPR, 2009.\n",
      "\n",
      "[Denton et al., 2014] Emily Denton, Wojciech Zaremba,\n",
      "Joan Bruna, Yann Lecun, and Rob Fergus. Exploiting\n",
      "linear structure within convolutional networks for efﬁcient\n",
      "evaluation. In NIPS, 2014.\n",
      "\n",
      "[Guo et al., 2018] Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong\n",
      "Chen, and Jianguo Li. Network decoupling: From regular\n",
      "to depthwise separable convolutions. In BMVC, 2018.\n",
      "[Han et al., 2015a] Song Han, Huizi Mao, and William J.\n",
      "Dally. Deep compression: Compressing deep neural net-\n",
      "work with pruning, trained quantization and huffman cod-\n",
      "ing. CoRR, abs/1510.00149, 2015.\n",
      "\n",
      "[Han et al., 2015b] Song Han, Jeff Pool, John Tran, and\n",
      "William Dally. Learning both weights and connections for\n",
      "efﬁcient neural network. In NIPS, 2015.\n",
      "\n",
      "[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\n",
      "Ren, and Jian Sun. Deep residual learning for image recog-\n",
      "nition. 2016.\n",
      "\n",
      "[He et al., 2017] Yihui He, Xiangyu Zhang, and Jian Sun.\n",
      "Channel pruning for accelerating very deep neural net-\n",
      "works. In ICCV, 2017.\n",
      "\n",
      "[Jaderberg et al., 2014] Max Jaderberg, Andrea Vedaldi, and\n",
      "Andrew Zisserman.\n",
      "Speeding up convolutional neu-\n",
      "ral networks with low rank expansions. arXiv preprint\n",
      "arXiv:1405.3866, 2014.\n",
      "\n",
      "[Krizhevsky and Hinton, 2009] Alex Krizhevsky and Geof-\n",
      "frey Hinton. Learning multiple layers of features from tiny\n",
      "images. Computer Science, 2009.\n",
      "[Li et al., 2016] Hao Li, Asim Kadav,\n",
      "\n",
      "Igor Durdanovic,\n",
      "Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for ef-\n",
      "ﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016.\n",
      "[Li et al., 2017] Hao Li, Soham De, Zheng Xu, Christoph\n",
      "Studer, Hanan Samet, and Tom Goldstein. Training quan-\n",
      "tized nets: A deeper understanding. In NIPS, 2017.\n",
      "\n",
      "[Liu et al., 2017] Zhuang Liu, Jianguo Li, Zhiqiang Shen,\n",
      "Gao Huang, Shoumeng Yan, and Changshui Zhang.\n",
      "\n",
      "Learning efﬁcient convolutional networks through net-\n",
      "work slimming. In ICCV, 2017.\n",
      "\n",
      "[Luo et al., 2017] Jian-Hao Luo, Jianxin Wu, and Weiyao\n",
      "Lin. Thinet: A ﬁlter level pruning method for deep neural\n",
      "network compression. ICCV, 2017.\n",
      "\n",
      "[Luo et al., 2018] Jian-Hao Luo, Hao Zhang, Hong-Yu\n",
      "Zhou, Chen-Wei Xie, Jianxin Wu, and Weiyao Lin. Thinet:\n",
      "pruning cnn ﬁlters for a thinner net. TPAMI, 2018.\n",
      "\n",
      "[Ma et al., 2018] Ningning Ma, Xiangyu Zhang, Hai-Tao\n",
      "Zheng, and Jian Sun. Shufﬂenet v2: Practical guide-\n",
      "lines for efﬁcient cnn architecture design. arXiv preprint\n",
      "arXiv:1807.11164, 2018.\n",
      "\n",
      "[Mirsky, 1960] Leon Mirsky. Symmetric gauge functions\n",
      "and unitarily invariant norms. The quarterly journal of\n",
      "mathematics, 11(1):50–59, 1960.\n",
      "\n",
      "[Rastegari et al., 2016] Mohammad Rastegari, Vicente Or-\n",
      "donez, Joseph Redmon, and Ali Farhadi. Xnor-net: Ima-\n",
      "genet classiﬁcation using binary convolutional neural net-\n",
      "works. In ECCV, 2016.\n",
      "\n",
      "[Sandler et al., 2018] Mark Sandler, Andrew Howard, Men-\n",
      "glong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\n",
      "Mobilenetv2: Inverted residuals and linear bottlenecks. In\n",
      "CVPR, June 2018.\n",
      "\n",
      "[Stewart, 1990] Gilbert W Stewart. Matrix perturbation the-\n",
      "\n",
      "ory. 1990.\n",
      "\n",
      "[Watson, 1992] G Alistair Watson. Characterization of the\n",
      "subdifferential of some matrix norms. Linear algebra and\n",
      "its applications, 170:33–45, 1992.\n",
      "\n",
      "[Wen et al., 2016] Wei Wen, Chunpeng Wu, Yandan Wang,\n",
      "Yiran Chen, and Hai Li. Learning structured sparsity in\n",
      "deep neural networks. In NIPS, 2016.\n",
      "\n",
      "[Wen et al., 2017] Wei Wen, Cong Xu, Chunpeng Wu, Yan-\n",
      "dan Wang, Yiran Chen, and Hai Li. Coordinating ﬁlters\n",
      "for faster deep neural networks. In ICCV, 2017.\n",
      "\n",
      "[Xu et al., 2018] Yuhui Xu, Yongzhuang Wang, Aojun\n",
      "Zhou, Weiyao Lin, and Hongkai Xiong. Deep neural net-\n",
      "work compression with single and multiple level quantiza-\n",
      "tion. CoRR, abs/1803.03289, 2018.\n",
      "\n",
      "[Xu et al., 2019] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen,\n",
      "Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, and\n",
      "Hongkai Xiong. Trained rank pruning for efﬁcient deep\n",
      "neural networks. In NIPS EMC2 workshop, 2019.\n",
      "\n",
      "[Zhang et al., 2016] Xiangyu Zhang, Jianhua Zou, Kaim-\n",
      "ing He, and Jian Sun. Accelerating very deep convolu-\n",
      "tional networks for classiﬁcation and detection. TPAMI,\n",
      "38(10):1943–1955, 2016.\n",
      "\n",
      "[Zhou et al., 2017] Aojun Zhou, Anbang Yao, Yiwen Guo,\n",
      "Lin Xu, and Yurong Chen. Incremental network quanti-\n",
      "zation: Towards lossless cnns with low-precision weights.\n",
      "arXiv preprint arXiv:1702.03044, 2017.\n",
      "\n",
      "[Zhou et al., 2019] Yuefu Zhou, Ya Zhang, Yanfeng Wang,\n",
      "and Qi Tian. Accelerate cnn via recursive bayesian prun-\n",
      "ing. In ICCV, 2019.\n",
      "\n",
      " model output {\"references\": [\n",
      "        { \"title\": \"Alvarez and Salzmann, 2017\", \"authors\": [\"Jose M Alvarez\", \"Mathieu Salzmann\"], \"date\": \"2017-05-24\", \"journal\": null },\n",
      "        { \"title\": \"Mirsky, 1960\", \"authors\": [\"Leon Mirsky\"], \"date\": \"1960\", \"journal\": null },\n",
      "        { \"title\": \"Wen et al., 2017\", \"authors\": [\"Wei Wen\", \"Cong Xu\", \"Yanping Wang\", \"Yiran Chen\", \"Hai Li\"], \"date\": \"2018-04-29\", \"journal\": null },\n",
      "        { \"title\": \"Ma et al., 2018\", \"authors\": [\"Ningning Ma\", \"Xiangyu Zhang\", \"Hai-Tao Zheng\", \"Jian Sun\"], \"date\": \"2018-07-25\", \"journal\": null },\n",
      "        { \"title\": \"Sandler et al., 2018\", \"authors\": [\"Mark Sandler\", \"Andrew Howard\", \"Menming Zhu\", \"Andrey Zhmoginov\", \"Liang-Chieh Chen\"], \"date\": \"2018-06-27\", \"journal\": null },\n",
      "        { \"title\": \"Wen et al., 2017\", \"authors\": [\"Wei Wen\", \"Cong Xu\", \"Yanping Wang\", \"Yiran Chen\", \"Hai Li\"], \"date\": \"2017-05-18\", \"journal\": null },\n",
      "        { \"title\": \"Luo et al., 2018\", \"authors\": [\"Jian-Hao Luo\", \"Hao Zhang\", \"Hong-Yu Zhou\", \"Chen-Wei Xie\", \"Jin-Xin Wu\"], \"date\": \"2018-03-27\", \"journal\": null },\n",
      "        { \"title\": \"Xu et al., 2019\", \"authors\": [\"Yuhui Xu\", \"Yuxi Li\", \"Shuai Zhang, ...], \"],\n",
      "    \"date\": \"2020-01-01\",\n",
      "    \"journal\": null }\n",
      "        ]\n",
      "    }\n",
      "     \t        \t\t\t\t\t\t\n",
      "titles_count 8\n",
      "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\n",
      "bonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n",
      "2019. Transformer-xl: Attentive language models\n",
      "In Proceedings of\n",
      "beyond a ﬁxed-length context.\n",
      "the 57th Conference of the Association for Compu-\n",
      "tational Linguistics, ACL 2019, Florence, Italy, July\n",
      "28- August 2, 2019, Volume 1: Long Papers, pages\n",
      "2978–2988.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2018. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understand-\n",
      "ing. ArXiv, abs/1810.04805.\n",
      "\n",
      "Mitchell A. Gordon, Kevin Duh, and Nicholas An-\n",
      "drews. 2020. Compressing BERT: studying the ef-\n",
      "fects of weight pruning on transfer learning. CoRR,\n",
      "abs/2002.08307.\n",
      "\n",
      "Fu-Ming Guo, Sijia Liu, Finlay S. Mungall, Xue\n",
      "Lin, and Yanzhi Wang. 2019. Reweighted proxi-\n",
      "mal pruning for large-scale language representation.\n",
      "CoRR, abs/1909.12486.\n",
      "\n",
      "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n",
      "2015. Distilling the knowledge in a neural network.\n",
      "ArXiv, abs/1503.02531.\n",
      "\n",
      "Jeremy Howard and Sebastian Ruder. 2018. Universal\n",
      "language model ﬁne-tuning for text classiﬁcation. In\n",
      "Proceedings of the 56th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics, ACL 2018,\n",
      "Melbourne, Australia, July 15-20, 2018, Volume 1:\n",
      "Long Papers, pages 328–339.\n",
      "\n",
      "Peter Izsak, Shira Guskin, and Moshe Wasserblat.\n",
      "2019. Training compact models for low resource\n",
      "entity tagging using pre-trained language models.\n",
      "CoRR, abs/1910.06294.\n",
      "\n",
      "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\n",
      "Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n",
      "2019. Tinybert: Distilling BERT for natural lan-\n",
      "guage understanding. CoRR, abs/1909.10351.\n",
      "\n",
      "Guillaume Lample and Alexis Conneau. 2019. Cross-\n",
      "CoRR,\n",
      "language model pretraining.\n",
      "\n",
      "lingual\n",
      "abs/1901.07291.\n",
      "\n",
      "Zhenzhong Lan, Mingda Chen, Sebastian Goodman,\n",
      "Kevin Gimpel, Piyush Sharma, and Radu Sori-\n",
      "cut. 2019.\n",
      "ALBERT: A lite BERT for self-\n",
      "supervised learning of language representations.\n",
      "CoRR, abs/1909.11942.\n",
      "\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Veselin Stoyanov, and Luke Zettlemoyer.\n",
      "2019. BART: denoising sequence-to-sequence pre-\n",
      "training for natural language generation, translation,\n",
      "and comprehension. CoRR, abs/1910.13461.\n",
      "\n",
      "Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher,\n",
      "and Caiming Xiong. 2019a. Attentive student meets\n",
      "multi-task teacher: Improved knowledge distillation\n",
      "for pretrained models. CoRR, abs/1911.03588.\n",
      "\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\n",
      "dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n",
      "Luke Zettlemoyer, and Veselin Stoyanov. 2019b.\n",
      "Roberta: A robustly optimized BERT pretraining\n",
      "approach. CoRR, abs/1907.11692.\n",
      "\n",
      "Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang,\n",
      "Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai\n",
      "Tong, and Jing Bai. 2020. Ladabert: Lightweight\n",
      "adaptation of BERT through hybrid model compres-\n",
      "sion. CoRR, abs/2004.04124.\n",
      "\n",
      "J. S. McCarley. 2019. Pruning a bert-based question\n",
      "\n",
      "answering model. CoRR, abs/1910.06360.\n",
      "\n",
      "\fPaul Michel, Omer Levy, and Graham Neubig. 2019.\n",
      "Are sixteen heads really better than one? CoRR,\n",
      "abs/1905.10650.\n",
      "\n",
      "Subhabrata Mukherjee and Ahmed Hassan Awadal-\n",
      "lah. 2019. Distilling transformers into simple neu-\n",
      "ral networks with unlabeled transfer data. CoRR,\n",
      "abs/1910.01769.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer,\n",
      "Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "Luke S. Zettlemoyer. 2018. Deep contextualized\n",
      "word representations. ArXiv, abs/1802.05365.\n",
      "\n",
      "Alec Radford. 2018. Improving language understand-\n",
      "\n",
      "ing by generative pre-training.\n",
      "\n",
      "Alec Radford, Jeff Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, and Ilya Sutskever. 2019. Language\n",
      "models are unsupervised multitask learners.\n",
      "\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter J. Liu. 2019. Exploring the limits\n",
      "of transfer learning with a uniﬁed text-to-text trans-\n",
      "former. CoRR, abs/1910.10683.\n",
      "\n",
      "Victor Sanh, Lysandre Debut, Julien Chaumond, and\n",
      "Thomas Wolf. 2019. Distilbert, a distilled version\n",
      "of BERT: smaller, faster, cheaper and lighter. CoRR,\n",
      "abs/1910.01108.\n",
      "\n",
      "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\n",
      "Yao, Amir Gholami, Michael W. Mahoney, and\n",
      "Kurt Keutzer. 2019. Q-BERT: hessian based ul-\n",
      "tra low precision quantization of BERT. CoRR,\n",
      "abs/1909.05840.\n",
      "\n",
      "Elena Voita, David Talbot, Fedor Moiseev, Rico Sen-\n",
      "nrich, and Ivan Titov. 2019. Analyzing multi-head\n",
      "self-attention: Specialized heads do the heavy lift-\n",
      "In Proceedings of the\n",
      "ing, the rest can be pruned.\n",
      "57th Conference of the Association for Computa-\n",
      "tional Linguistics, ACL 2019, Florence, Italy, July\n",
      "28- August 2, 2019, Volume 1: Long Papers, pages\n",
      "5797–5808.\n",
      "\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Fe-\n",
      "lix Hill, Omer Levy, and Samuel R. Bowman.\n",
      "2018. GLUE: A multi-task benchmark and anal-\n",
      "ysis platform for natural\n",
      "language understand-\n",
      "In Proceedings of the Workshop: Analyz-\n",
      "ing.\n",
      "ing and Interpreting Neural Networks for NLP,\n",
      "BlackboxNLP@EMNLP 2018, Brussels, Belgium,\n",
      "November 1, 2018, pages 353–355.\n",
      "\n",
      "Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019.\n",
      "Structured pruning of large language models. CoRR,\n",
      "abs/1910.04732.\n",
      "\n",
      "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\n",
      "and Ming Zhou. 2020. Bert-of-theseus: Compress-\n",
      "ing BERT by progressive module replacing. CoRR,\n",
      "abs/2002.02925.\n",
      "\n",
      "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\n",
      "Carbonell, Ruslan Salakhutdinov, and Quoc V.\n",
      "Xlnet: Generalized autoregressive\n",
      "Le. 2019.\n",
      "CoRR,\n",
      "pretraining for language understanding.\n",
      "abs/1906.08237.\n",
      "\n",
      "Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\n",
      "Wasserblat. 2019. Q8BERT: quantized 8bit BERT.\n",
      "CoRR, abs/1910.06188.\n",
      "\n",
      "G. W. Stewart. 1991. Perturbation theory for the sin-\n",
      "gular value decomposition. SVD and Signal Pro-\n",
      "cessing, II: Algorithms, Analysis and Applications,\n",
      "pages 99–109.\n",
      "\n",
      "Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny\n",
      "Zhou. 2019. Extreme language model compres-\n",
      "sion with optimal subwords and shared projections.\n",
      "CoRR, abs/1909.11687.\n",
      "\n",
      "Emma Strubell, Ananya Ganesh, and Andrew McCal-\n",
      "lum. 2019. Energy and policy considerations for\n",
      "In Proceedings of the 57th\n",
      "deep learning in NLP.\n",
      "Conference of the Association for Computational\n",
      "Linguistics, ACL 2019, Florence, Italy, July 28- Au-\n",
      "gust 2, 2019, Volume 1: Long Papers, pages 3645–\n",
      "3650.\n",
      "\n",
      "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\n",
      "Patient knowledge distillation for BERT model com-\n",
      "pression.\n",
      "\n",
      "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\n",
      "Vechtomova, and Jimmy Lin. 2019. Distilling task-\n",
      "speciﬁc knowledge from BERT into simple neural\n",
      "networks. CoRR, abs/1903.12136.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In Advances in Neural Information Pro-\n",
      "cessing Systems 30: Annual Conference on Neural\n",
      "Information Processing Systems 2017, 4-9 Decem-\n",
      "ber 2017, Long Beach, CA, USA, pages 5998–6008.\n",
      "\n",
      " model output {\"references\": [\n",
      "        {\n",
      "            \"title\": \"Voita et al. (2019)\",\n",
      "            \"authors\": [\"Zhilin Yang\", \"Zihang Dai\", \"Yiming Yang\", \"Jaime G.\",\n",
      "              \"Carbonell\", \"Ruslan Salakhutdinov\", \"Quoc V. Xlnet\"],\n",
      "            \"date\": \"2019\",\n",
      "            \"journal\": null\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Wasserblat and Zafrir (2019)\",\n",
      "            \"authors\": [\"Moshe Wasserblat\", \"Guy Boudoukh\", \"Peter Izsak\", \"...\"\n",
      "            ], \"date\": \"2019\",\n",
      "            \"journal\": null\n",
      "        }\n",
      "    ]}\n",
      "    \n",
      "titles_count 2\n",
      "Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical commonsense in natural\n",
      "language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages\n",
      "7432–7439, 2020.\n",
      "\n",
      "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n",
      "G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\n",
      "processing systems, 33:1877–1901, 2020.\n",
      "\n",
      "W. Cheng, Y. Cai, K. Lv, and H. Shen. Teq: Trainable equivalent transformation for quantization of\n",
      "\n",
      "llms. arXiv preprint arXiv:2310.10944, 2023a.\n",
      "\n",
      "W. Cheng, W. Zhang, H. Shen, Y. Cai, X. He, and K. Lv. Optimize weight rounding via signed\n",
      "\n",
      "gradient descent for the quantization of llms. arXiv preprint arXiv:2309.05516, 2023b.\n",
      "\n",
      "T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for\n",
      "\n",
      "transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n",
      "\n",
      "T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized\n",
      "\n",
      "llms. arXiv preprint arXiv:2305.14314, 2023.\n",
      "\n",
      "E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for\n",
      "\n",
      "generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n",
      "\n",
      "S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning,\n",
      "trained quantization and huffman coding, 2015. URL https://arxiv.org/abs/1510.00149.\n",
      "\n",
      "B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko.\n",
      "Quantization and training of neural networks for efficient integer-arithmetic-only inference. In\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June\n",
      "2018.\n",
      "\n",
      "S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer. Squeezellm:\n",
      "\n",
      "Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.\n",
      "\n",
      "A. Kuzmin, M. Van Baalen, Y. Ren, M. Nagel, J. Peters, and T. Blankevoort. Fp8 quantization: The\n",
      "power of the exponent. Advances in Neural Information Processing Systems, 35:14651–14662,\n",
      "2022.\n",
      "\n",
      "R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim,\n",
      "\n",
      "et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.\n",
      "\n",
      "J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quantization\n",
      "\n",
      "for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n",
      "\n",
      "P. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite, S. Ha, A. Heinecke,\n",
      "P. Judd, J. Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.\n",
      "\n",
      "D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,\n",
      "and R. Fernández. The lambada dataset: Word prediction requiring a broad discourse context.\n",
      "arXiv preprint arXiv:1606.06031, 2016.\n",
      "\n",
      "A. Rodriguez, E. Segal, E. Meiri, E. Fomenko, Y. J. Kim, H. Shen, and B. Ziv. Lower numerical\n",
      "\n",
      "precision deep learning inference and training. Intel White Paper, 3(1):19, 2018.\n",
      "\n",
      "B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,\n",
      "et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\n",
      "\n",
      "K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd\n",
      "\n",
      "schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\n",
      "\n",
      "H. Shen, N. Mellempudi, X. He, Q. Gao, C. Wang, and M. Wang. Efficient post-training quantization\n",
      "\n",
      "with fp8 formats, 2023.\n",
      "\n",
      "5\n",
      "\n",
      "\fX. Sun, J. Choi, C.-Y. Chen, N. Wang, S. Venkataramani, V. V. Srinivasan, X. Cui, W. Zhang, and\n",
      "K. Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural\n",
      "networks. Advances in neural information processing systems, 32, 2019.\n",
      "\n",
      "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\n",
      "E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971, 2023a.\n",
      "\n",
      "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\n",
      "P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\n",
      "preprint arXiv:2307.09288, 2023b.\n",
      "\n",
      "V. Vanhoucke, A. Senior, and M. Z. Mao. Improving the speed of neural networks on cpus. In Deep\n",
      "\n",
      "Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.\n",
      "\n",
      "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\n",
      "\n",
      "Attention is all you need. Advances in neural information processing systems, 30, 2017.\n",
      "\n",
      "X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, and X. Liu. Outlier suppression+: Accurate\n",
      "quantization of large language models by equivalent and optimal shifting and scaling. arXiv\n",
      "preprint arXiv:2304.09145, 2023.\n",
      "\n",
      "X. Wu, Z. Yao, and Y. He. Zeroquant-fp: A leap forward in llms post-training w4a8 quantization\n",
      "\n",
      "using floating-point formats. arXiv preprint arXiv:2307.09782, 2023.\n",
      "\n",
      "G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\n",
      "post-training quantization for large language models. In International Conference on Machine\n",
      "Learning, pages 38087–38099. PMLR, 2023.\n",
      "\n",
      "R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish\n",
      "\n",
      "your sentence? arXiv preprint arXiv:1905.07830, 2019.\n",
      "\n",
      "S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin,\n",
      "et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n",
      "\n",
      "6\n",
      "\n",
      " model output {\"references\": [\n",
      "        {\"title\": \"Llama 3: Open foundation model with fine-tuned chat models. arXiv preprint arXiv:2308.12950, 2023a\", \"authors\": [\"K. Sakaguchi\", \"R. L. Bras\", \"C. Bhagavatula\", \"Y. Choi\"], \"date\": \"2023a\", \"journal\": \"arXiv preprint arXiv:2308.12950, 2023a\"},\n",
      "        {\"title\": \"Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b\", \"authors\": [\"H. Touvron, L. Martin\", \"K. Stone, P. Albert\", \"A. Almahairi, Y. Babaei\"], \"date\": \"2023b\", \"journal\": \"arXiv preprint arXiv:2307.09288, 2023b\"},\n",
      "        {\"title\": \"H. Shen, N. Mellepudi, X. He, Q. Gao, C. Wang, and M. W. Wang. Efficient post-training quantization with fp8 formats, 2023.\", \"authors\": [\"K. Sakaguchi\", \"R. L. Bras\", \"C. Bhagavatula\", \"Y. Choi\"], \"date\": \"2023.\", \"journal\": \"arXiv preprint arXiv:2308.12950, 2023.\"}\n",
      "    ]}\n",
      "    \t\t\t\t\t\t\t\t    \t  \t\n",
      "titles_count 3\n",
      "over a list of items, microarray (gene expression) mea-\n",
      "surements, and collections of images. Consider, for ex-\n",
      "ample, a dataset of user preferences for movies or jokes.\n",
      "The premise behind a factor model is that there is only\n",
      "a small number of factors inﬂuencing the preferences,\n",
      "and that a user’s preference vector is determined by\n",
      "how each factor applies to that user. In a linear fac-\n",
      "tor model, each factor is a preference vector, and a\n",
      "user’s preferences correspond to a linear combination\n",
      "of these factor vectors, with user-speciﬁc coeﬃcients.\n",
      "Thus, for n users and d items, the preferences accord-\n",
      "ing to a k-factor model are given by the product of\n",
      "an n × k coeﬃcient matrix (each row representing the\n",
      "extent to which each factor is used) and a k × d fac-\n",
      "tor matrix whose rows are the factors. The preference\n",
      "matrices which admit such a factorization are matri-\n",
      "ces of rank at most k. Thus, training such a linear\n",
      "factor model amounts to approximating the empirical\n",
      "preferences with a low-rank matrix.\n",
      "\n",
      "Low-rank matrix approximation with respect to the\n",
      "Frobenius norm—minimizing the sum squared diﬀer-\n",
      "ences to the target matrix—can be easily solved with\n",
      "Singular Value Decomposition (SVD). For many ap-\n",
      "plications, however, the deviation between the ob-\n",
      "served matrix and the low-rank approximation should\n",
      "be measured relative to a weighted (or other) norm.\n",
      "While the extension to the weighted-norm case is con-\n",
      "ceptually straightforward, and dates back to early\n",
      "work on factor analysis (Young, 1940), standard algo-\n",
      "rithms (such as SVD) for solving the unweighted case\n",
      "do not carry over to the weighted case.\n",
      "\n",
      "Weighted norms can arise in a number of situations.\n",
      "Zero/one weights, for example, arise when some of the\n",
      "entries in the matrix are not observed. More generally,\n",
      "we may introduce weights in response to some exter-\n",
      "nal estimate of the noise variance associated with each\n",
      "measurement. This is the case, for example, in gene ex-\n",
      "pression analysis, where the error model for microarray\n",
      "measurements provides entry-speciﬁc noise estimates.\n",
      "Setting the weights inversely proportional to the as-\n",
      "sumed noise variance can lead to a better reconstruc-\n",
      "tion of the underlying structure. In other applications,\n",
      "entries in the target matrix may represent aggregates\n",
      "of many samples. The standard unweighted low-rank\n",
      "approximation (e.g., for separating style and content\n",
      "(Tenenbaum & Freeman, 2000)) would in this context\n",
      "assume that the number of samples is uniform across\n",
      "the entries. Non-uniform weights are needed to appro-\n",
      "priately capture any diﬀerences in the sample sizes.\n",
      "\n",
      "Despite its usefulness, the weighted extension has at-\n",
      "tracted relatively little attention. Shpak (1990) and Lu\n",
      "et al. (1997) studied weighted-norm low-rank approxi-\n",
      "mations for the design of two-dimensional digital ﬁlters\n",
      "where the weights arise from constraints of varying im-\n",
      "portance. Shpak developed gradient-based optimiza-\n",
      "tion methods while Lu et al. suggested alternating-\n",
      "optimization methods. In both cases, rank-k approx-\n",
      "imations are greedily combined from k rank-one ap-\n",
      "\n",
      "Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003), Washington DC, 2003.\n",
      "\n",
      "\fproximations. Unlike for the unweighted case, such a\n",
      "greedy procedure is sub-optimal.\n",
      "\n",
      "We suggest optimization methods that are signiﬁ-\n",
      "cantly more eﬃcient and simpler to implement (Sec-\n",
      "tion 2). We also consider other measures of deviation,\n",
      "beyond weighted Frobenius norms. Such measures\n",
      "arise, for example, when the noise model associated\n",
      "with matrix elements is known but not is Gaussian.\n",
      "For example, for binary data, a logistic model with an\n",
      "underlying low-rank representation might be more ap-\n",
      "propriate. In Section 3 we show how weighted-norm\n",
      "approximation problems arise as subroutines for solv-\n",
      "ing such a low-rank problem. Finally, in Section 4, we\n",
      "illustrate the use of these methods by applying them\n",
      "to a collaborative ﬁltering problem.\n",
      "\n",
      "2. Weighted Low-Rank Approximations\n",
      "\n",
      "Given a target matrix A ∈ <n×d, a corresponding non-\n",
      "negative weight matrix W ∈ <n×d\n",
      "+ , and a desired (inte-\n",
      "ger) rank k, we would like to ﬁnd a matrix X ∈ <n×d of\n",
      "rank (at most) k, that minimizes the weighted Frobe-\n",
      "nius distance J(X) = P\n",
      "i,a Wi,a (Xi,a − Ai,a)2. In this\n",
      "section, we analyze this optimization problem and con-\n",
      "sider optimization methods for it.\n",
      "\n",
      "2.1. A Matrix-Factorization View\n",
      "\n",
      "It will be useful to consider the decomposition X =\n",
      "U V 0 where U ∈ <n×k and V ∈ <d×k. Since any rank-\n",
      "k matrix can be decomposed in such a way, and any\n",
      "pair of such matrices yields a rank-k matrix, we can\n",
      "think of the problem as an unconstrained minimiza-\n",
      "tion problem over pairs of matrices (U, V ) with the\n",
      "minimization objective\n",
      "\n",
      "J(U, V ) =\n",
      "\n",
      "X\n",
      "\n",
      "i,a\n",
      "\n",
      "Wi,a (Ai,a − (U V 0)i,a)2\n",
      "\n",
      "X\n",
      "\n",
      "=\n",
      "\n",
      "i,a\n",
      "\n",
      "Wi,a\n",
      "\n",
      "Ai,a −\n",
      "\n",
      "!2\n",
      "\n",
      "Ui,αVa,α\n",
      "\n",
      ".\n",
      "\n",
      "X\n",
      "\n",
      "α\n",
      "\n",
      "This decomposition is not unique. For any invertible\n",
      "R ∈ <k×k, the pair (U R, V R−1) provides a factoriza-\n",
      "tion equivalent to (U, V ), i.e. J(U, V ) = J(U R, V R−1),\n",
      "resulting in a k2-dimensional manifold of equivalent so-\n",
      "lutions1. In particular, any (non-degenerate) solution\n",
      "(U, V ) can be orthogonalized to a (non-unique) equiv-\n",
      "alent orthogonal solution ¯U = U R, ¯V = V R−1 such\n",
      "that ¯V 0 ¯V = I and ¯U 0 ¯U is a diagonal matrix.2\n",
      "\n",
      "1An equivalence class of solutions actually consists of a\n",
      "collection of such manifolds, asymptotically tangent to one\n",
      "another.\n",
      "\n",
      "2We slightly abuse the standard linear-algebra notion of\n",
      "\n",
      "We ﬁrst revisit the well-studied case where all weights\n",
      "are equal to one. It is a standard result that the low-\n",
      "rank matrix minimizing the unweighted sum-squared\n",
      "distance to A is given by the leading components of\n",
      "the singular value decomposition of A. It will be in-\n",
      "structive to consider this case carefully and understand\n",
      "why the unweighted low-rank approximation has such\n",
      "a clean and easily computable form. We will then be\n",
      "able to move on to the weighted case, and understand\n",
      "how, and why, the situation becomes less favorable.\n",
      "\n",
      "∂J\n",
      "\n",
      "In the unweighted case, the partial derivatives of the\n",
      "objective J with respect to U, V are ∂J\n",
      "∂U = 2(U V 0 −\n",
      "∂V = 2(V U 0 − A0)U . Solving ∂J\n",
      "A)V , ∂J\n",
      "∂U = 0 for U\n",
      "yields U = AV (V 0V )−1; focusing on an orthogonal\n",
      "solution, where V 0V = I and U 0U = Λ is diagonal,\n",
      "yields U = AV . Substituting back into ∂J\n",
      "∂V = 0, we\n",
      "have 0 = V U 0U − A0U = V Λ − A0AV . The columns\n",
      "of V are mapped by A0A to multiples of themselves,\n",
      "i.e. they are eigenvectors of A0A. Thus, the gradient\n",
      "∂(U,V ) vanishes at an orthogonal (U, V ) if and only\n",
      "if the columns of V are eigenvectors of A0A and the\n",
      "columns of U are corresponding eigenvectors of AA0,\n",
      "scaled by the square root of their eigenvalues. More\n",
      "generally, the gradient vanishes at any (U, V ) if and\n",
      "only if the columns of U are spanned by eigenvec-\n",
      "tors of AA0 and the columns of V are correspondingly\n",
      "spanned by eigenvectors of A0A. In terms of the sin-\n",
      "gular value decomposition A = U0SV 0\n",
      "0 , the gradient\n",
      "vanishes at (U, V ) if and only if there exist matrices\n",
      "Q0\n",
      "U QV = I ∈ <k×k (or more generally, a zero/one di-\n",
      "agonal matrix rather than I) such that U = U0SQU ,\n",
      "V = V0QV . This provides a complete characterization\n",
      "of the critical points of J. We now turn to identifying\n",
      "the global minimum and understanding the nature of\n",
      "the remaining critical points.\n",
      "\n",
      "The global minimum can be identiﬁed by investigat-\n",
      "ing the value of the objective function at the criti-\n",
      "cal points. Let σ1 ≥ · · · ≥ σm be the eigenvalues of\n",
      "A0A. For critical (U, V ) that are spanned by eigen-\n",
      "vectors corresponding to eigenvalues {σq|q ∈ Q}, the\n",
      "error of J(U, V ) is given by the sum of the eigenval-\n",
      "ues not in Q (P\n",
      "q6∈Q σq), and so the global minimum is\n",
      "attained when the eigenvectors corresponding to the\n",
      "highest eigenvalues are taken. As long as there are\n",
      "no repeated eigenvalues, all (U, V ) global minima cor-\n",
      "respond to the same low-rank matrix X = U V 0, and\n",
      "belong to the same equivalence class. 3\n",
      "\n",
      "“orthogonal” since we cannot always have both ¯U 0 ¯U = I\n",
      "and ¯V 0 ¯V = I.\n",
      "\n",
      "3If there are repeated eigenvalues, the global minima\n",
      "correspond to a polytope of low-rank approximations in\n",
      "X space; in U, V space, they form a collection of higher-\n",
      "dimensional asymptotically tangent manifolds.\n",
      "\n",
      "\f2\n",
      "\n",
      "q∈Q q − (cid:0)k\n",
      "\n",
      "In order to understand the behavior of the objective\n",
      "function, it is important to study the remaining critical\n",
      "points. For a critical point (U, V ) spanned by eigen-\n",
      "vectors corresponding to eigenvalues as above (assum-\n",
      "ing no repeated eigenvalues), the Hessian has exactly\n",
      "(cid:1) negative eigenvalues: we can replace any\n",
      "P\n",
      "eigencomponent with eigenvalue σ with an alternate\n",
      "eigencomponent not already in (U, V ) with eigenvalue\n",
      "σ0 > σ, decreasing the objective function. The change\n",
      "can be done gradually, replacing the component with a\n",
      "convex combination of the original and improved com-\n",
      "ponents. This results in a line between the two critical\n",
      "points which is a monotonic improvement path. Since\n",
      "(cid:1) such pairs of eigencomponents,\n",
      "there are P\n",
      "there are at least this many directions of improve-\n",
      "ment. Other than these directions of improvement,\n",
      "and the k2 directions along the equivalence manifold\n",
      "corresponding to the k2 zero eigenvalues of the Hes-\n",
      "sian, all other eigenvalues of the Hessian are positive\n",
      "(or zero, in very degenerate A).\n",
      "\n",
      "q∈Q q − (cid:0)k\n",
      "\n",
      "2\n",
      "\n",
      "Hence, in the unweighted case, all critical points that\n",
      "are not global minima are saddle points. This is an\n",
      "important observation: Despite J(U, V ) not being a\n",
      "convex function, all of its local minima are global.\n",
      "\n",
      "We now move on to the weighted case, and try to take\n",
      "the same path. Unfortunately, when weights are in-\n",
      "troduced, the critical point structure changes signiﬁ-\n",
      "cantly.\n",
      "\n",
      "The partial derivatives become (with ⊗ denoting\n",
      "element-wise multiplication):\n",
      "\n",
      "∂J\n",
      "\n",
      "∂U = 2(W ⊗ (U V 0 − A))V\n",
      "∂V = 2(W ⊗ (V U 0 − A0))U\n",
      "\n",
      "∂J\n",
      "\n",
      "The equation ∂J\n",
      "∂U = 0 is still a linear system in\n",
      "it can be solved, recovering\n",
      "U , and for a ﬁxed V ,\n",
      "U ∗\n",
      "V = arg minU J(U, V ) (since J(U, V ) is convex in\n",
      "U ). However, the solution cannot be written using a\n",
      "single pseudo-inverse V (V 0V )−1. Instead, a separate\n",
      "V )i of U ∗\n",
      "pseudo-inverse is required for each row (U ∗\n",
      "V :\n",
      "\n",
      "When W is of rank one, such concurrent diagonaliza-\n",
      "tion is possible, allowing for the same structure as in\n",
      "the unweighted case, and in particular an eigenvector-\n",
      "based solution (Irani & Anandan, 2000). However, for\n",
      "higher-rank W , we cannot achieve this concurrently for\n",
      "all rows. The critical points of the weighted low-rank\n",
      "approximation problem, therefore, lack the eigenvector\n",
      "structure of the unweighted case. Another implication\n",
      "of this is that the incremental structure of unweighted\n",
      "low-rank approximations is lost: an optimal rank-k\n",
      "factorization cannot necessarily be extended to an op-\n",
      "timal rank-(k + 1) factorization.\n",
      "\n",
      "Lacking an analytic solution, we revert to numerical\n",
      "optimization methods to minimize J(U, V ). But in-\n",
      "stead of optimizing J(U, V ) by numerically searching\n",
      "over (U, V ) pairs, we can take advantage of the fact\n",
      "that for a ﬁxed V , we can calculate U ∗\n",
      "V , and therefore\n",
      "also the projected objective J ∗(V ) = minU J(U, V ) =\n",
      "V , V ). The parameter space of J ∗(V ) is of course\n",
      "J(U ∗\n",
      "much smaller than that of J(U, V ), making optimiza-\n",
      "tion of J ∗(V ) more tractable. This is especially true\n",
      "in many typical applications where the the dimensions\n",
      "of A are highly skewed, with one dimension several or-\n",
      "ders of magnitude larger than the other (e.g. in gene\n",
      "expression analysis one often deals with thousands of\n",
      "genes, but only a few dozen experiments).\n",
      "\n",
      "Recovering U ∗\n",
      "V using (1) requires n inversions of k × k\n",
      "matrices. The dominating factor is actually the ma-\n",
      "trix multiplications: Each calculation of V 0WiV re-\n",
      "quires O(dk2) operations, for a total of O(ndk2) oper-\n",
      "ations. Although more involved than the unweighted\n",
      "case, this is still signiﬁcantly less than the prohibitive\n",
      "O(n3k3) required for each iteration suggested by Lu\n",
      "et al. (1997), or for Hessian methods on (U, V ) (Sh-\n",
      "pak, 1990), and is only a factor of k larger than the\n",
      "O(ndk) required just to compute the prediction U V 0.\n",
      "\n",
      "After recovering U ∗\n",
      "V , we can easily compute not only\n",
      "the value of the projected objective, but also its gra-\n",
      "dient. Since ∂J(V,U )\n",
      "\n",
      "= 0, we have\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)U =U ∗\n",
      "\n",
      "V\n",
      "\n",
      "∂U\n",
      "\n",
      "(U ∗\n",
      "\n",
      "V )i = (V 0WiV )−1V 0WiAi\n",
      "q\n",
      "\n",
      "q\n",
      "\n",
      "= pinv(\n",
      "\n",
      "WiV )(\n",
      "\n",
      "WiAi)\n",
      "\n",
      "(1)\n",
      "\n",
      "∂J ∗(V )\n",
      "\n",
      "∂V = ∂J(V,U )\n",
      "\n",
      "∂V\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)U =U ∗\n",
      "\n",
      "V\n",
      "\n",
      "= 2(W ⊗ (V U ∗\n",
      "V\n",
      "\n",
      "0 − A0))U ∗\n",
      "V .\n",
      "\n",
      "where Wi ∈ <k×k is a diagonal matrix with the weights\n",
      "from the ith row of W on the diagonal, and Ai is the\n",
      "ith row of the target matrix4. In order to proceed as\n",
      "in the unweighted case, we would have liked to choose\n",
      "V such that V 0WiV = I (or is at least diagonal). This\n",
      "can certainly be done for a single i, but in order to pro-\n",
      "ceed we need to diagonalize all V 0WiV concurrently.\n",
      "\n",
      "4Here and throughout the paper, rows of matrices, such\n",
      "V )i, are treated in equations as column vectors.\n",
      "\n",
      "as Ai and (U ∗\n",
      "\n",
      "The computation requires only O(ndk) operations,\n",
      "and is therefore “free” after U ∗\n",
      "\n",
      "V has been recovered.\n",
      "\n",
      "Equipped with the above calculations, we can use stan-\n",
      "dard gradient-descent techniques to optimize J ∗(V ).\n",
      "Unfortunately, though, unlike in the unweighted case,\n",
      "J(U, V ), and J ∗(V ), might have local minima that\n",
      "are not global. Figure 1 shows the emergence of a\n",
      "non-global local minimum of J ∗(V ) for a rank-one ap-\n",
      "proximation of A = (cid:0) 1 1.1\n",
      "(cid:1). The matrix V is a two-\n",
      "dimensional vector. But since J ∗(V ) is invariant under\n",
      "\n",
      "1 −1\n",
      "\n",
      "\finvertible scalings, V can be speciﬁed as an angle θ on\n",
      "a semi-circle. We plot the value of J ∗([cos θ, sin θ]) for\n",
      "each θ, and for varying weight matrices of the form\n",
      "W = (cid:0) 1+α 1\n",
      "(cid:1). At the front of the plot, the weight\n",
      "matrix is uniform and indeed there is only a single lo-\n",
      "cal minimum, but at the back of the plot, where the\n",
      "weight matrix emphasizes the diagonal, a non-global\n",
      "local minimum emerges.\n",
      "\n",
      "1+α\n",
      "\n",
      "1\n",
      "\n",
      "Figure 1. Emergence of local minima when the weights be-\n",
      "come non-uniform.\n",
      "\n",
      "Despite the abundance of local minima, we found gra-\n",
      "dient descent methods on J ∗(V ), and in particular con-\n",
      "jugate gradient descent, equipped with a long-range\n",
      "line-search for choosing the step size, very eﬀective in\n",
      "avoiding local minima and quickly converging to the\n",
      "global minimum.\n",
      "\n",
      "2.2. A Missing-Values View and an EM\n",
      "\n",
      "Procedure\n",
      "\n",
      "In this section we present an alternative optimiza-\n",
      "tion procedure, which is much simpler to implement.\n",
      "This procedure is based on viewing the weighted low-\n",
      "rank approximation problem as a maximum-likelihood\n",
      "problem with missing values.\n",
      "\n",
      "Consider ﬁrst systems with only zero/one weights,\n",
      "where only some of the elements of the target matrix A\n",
      "are observed (those with weight one) while others are\n",
      "missing (those with weight zero). Referring to a prob-\n",
      "abilistic model parameterized by a low-rank matrix X,\n",
      "where A = X + Z and Z is white Gaussian noise, the\n",
      "weighted cost of X is equivalent to the log-likelihood\n",
      "of the observed variables.\n",
      "\n",
      "This suggests an Expectation-Maximization proce-\n",
      "dure.\n",
      "In each EM update we would like to ﬁnd a\n",
      "new parameter matrix maximizing the expected log-\n",
      "\n",
      "likelihood of a ﬁlled-in A, where missing values are\n",
      "ﬁlled in according to the distribution imposed by the\n",
      "current estimate of X. This maximum-likelihood pa-\n",
      "rameter matrix is the (unweighted) low-rank approxi-\n",
      "mation of the mean ﬁlled-in A, which is A with miss-\n",
      "in the\n",
      "ing values ﬁlled in from X. To summarize:\n",
      "Expectation step values from the current estimate of\n",
      "X are ﬁlled in for the missing values in A, and in the\n",
      "Maximization step X is reestimated as a low-rank ap-\n",
      "proximation of the ﬁlled-in A.\n",
      "\n",
      "In order to extend this approach to a general weight\n",
      "matrix, consider a probabilistic system with several\n",
      "target matrices, A(1), A(2), . . . , A(N ), but with a single\n",
      "low-rank parameter matrix X, where A(r) = X + Z(r)\n",
      "and the random matrices Z(r) are independent white\n",
      "Gaussian noise with ﬁxed variance. When all target\n",
      "matrices are fully observed, the maximum likelihood\n",
      "setting for X is the low-rank approximation of the their\n",
      "average. Now, if some of the entries of some of the tar-\n",
      "get matrices are not observed, we can use a similar EM\n",
      "procedure, where in the expectation step values from\n",
      "the current estimate of X are ﬁlled in for all missing\n",
      "entries in the target matrices, and in the maximization\n",
      "step X is updated to be a low-rank approximation of\n",
      "the mean of the ﬁlled-in target matrices.\n",
      "\n",
      "To see how to use the above procedure to solve\n",
      "weighted low-rank approximation problems, consider\n",
      "systems with weights limited to Wia = wia\n",
      "N with inte-\n",
      "ger wia ∈ {0, 1, . . . , N }. Such a low-rank approxima-\n",
      "tion problem can be transformed to a missing value\n",
      "problem of the form above by “observing” the value\n",
      "Aia in wia of the target matrices (for each entry i, a),\n",
      "and leaving the entry as missing in the rest of the tar-\n",
      "get matrices. The EM update then becomes:\n",
      "\n",
      "X (t+1) = LRAk\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "W ⊗ A + (1 − W ) ⊗ X (t)(cid:17)\n",
      "\n",
      "(2)\n",
      "\n",
      "where LRAk(X) is the unweighted rank-k approxima-\n",
      "tion of X, as can be computed from the SVD. Note\n",
      "that this procedure is independent of N . For any\n",
      "weight matrix (scaled to weights between zero and\n",
      "one) the procedure in equation (2) can thus be seen\n",
      "as an expectation-maximization procedure. This pro-\n",
      "vides for a very simple, tweaking-free method for ﬁnd-\n",
      "ing weighted low-rank approximations.\n",
      "\n",
      "Although we found this EM-inspired method eﬀective\n",
      "in many cases, in some other cases the procedure con-\n",
      "verges to a local minimum which is not global. Since\n",
      "the method is completely deterministic, initialization\n",
      "of X plays a crucial role in promoting convergence to\n",
      "a global, or at least deep local, minimum, as well as\n",
      "the speed with which convergence is attained.\n",
      "\n",
      "Two obvious initialization methods are to initialize X\n",
      "\n",
      "0pi/2pi00.5122.53qaJ*(cos q, sin q) for W = 1 + a I\fto A, and to initialize X to zero.\n",
      "Initializing X to\n",
      "A works reasonably well if the weights are bounded\n",
      "away from zero, or if the target values in A have rela-\n",
      "tively small variance. However, when the weights are\n",
      "zero, or very close to zero, the target values become\n",
      "meaningless, and can throw oﬀ the search. Initializing\n",
      "X to zero avoids this problem, as target values with\n",
      "zero weights are completely ignored (as they should\n",
      "be), and works well as long as the weights are fairly\n",
      "dense. However, when the weights are sparse, it often\n",
      "converges to local minima which consistently under-\n",
      "predict the magnitude of the target values.\n",
      "\n",
      "As an alternative to these initialization methods, we\n",
      "found the following procedure very eﬀective: we initial-\n",
      "ize X to zero, but instead of seeking a rank-k approx-\n",
      "imation right away, we start with a full rank matrix,\n",
      "and gradually reduce the rank of our approximations.\n",
      "That is, the ﬁrst d − k iterations take the form:\n",
      "\n",
      "X (t+1) = LRAd−t\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "W ⊗ A + (1 − W ) ⊗ X (t)(cid:17)\n",
      "\n",
      ",\n",
      "\n",
      "(3)\n",
      "\n",
      "resulting in X (t) of rank (d−t+1). After reaching rank\n",
      "k, we revert back to the iterations of equation (2) un-\n",
      "til convergence. Note that with iterations of the form\n",
      "X (t+1) = W ⊗ A + (1 − W ) ⊗ X (t), without rank reduc-\n",
      "tions, we would have X (t)\n",
      "ia = (1 − (1 − Wia)t))Aia →\n",
      "(1 − e−tWia)Aia, which converges exponentially fast\n",
      "to A for positive weights. Of course, because of the\n",
      "rank reduction, this does not hold, but even the few\n",
      "high-rank iterations set values with weights away from\n",
      "zero close to their target values, as long as they do not\n",
      "signiﬁcantly contradict other values.\n",
      "\n",
      "2.3. Reconstruction Experiments\n",
      "\n",
      "Since the unweighted or simple low-rank approxima-\n",
      "tion problem permits a closed-form solution, one might\n",
      "be tempted to use such a solution even in the presence\n",
      "of non-uniform weights (i.e., ignore the weights). We\n",
      "demonstrate here that this procedure results in a sub-\n",
      "stantial loss of reconstruction accuracy as compared to\n",
      "the EM algorithm designed for the weighted problem.\n",
      "\n",
      "To this end, we generated 1000 × 30 low rank ma-\n",
      "trices combined with Gaussian noise models to yield\n",
      "the observed (target) matrices. For each matrix entry,\n",
      "the noise variance σ2\n",
      "ia was chosen uniformly in some\n",
      "noise level range characterized by a noise spread ratio\n",
      "max σ2/ min σ2. The planted matrix was subsequently\n",
      "reconstructed using both a weighted low-rank approx-\n",
      "imation with weights Wia = 1/σ2\n",
      "ia, and an unweighted\n",
      "low-rank approximation (using SVD). The quality of\n",
      "reconstruction was assessed by an unweighted squared\n",
      "distance from the “planted” matrix.\n",
      "\n",
      "Figure 2. Reconstruction of a 1000 × 30 rank-three matrix.\n",
      "Left: (a) weighted and unweighted reconstruction with a\n",
      "noise spread of 100 ; right: (b) reduction in reconstruction\n",
      "error for various noise spreads.\n",
      "\n",
      "Figure 2(a) shows the quality of reconstruction at-\n",
      "tained by the two approaches as a function of the\n",
      "signal (weighted variance of planted low-rank matrix)\n",
      "to noise (average noise variance) ratio, for a noise\n",
      "spread ratio of 100 (corresponding to weights in the\n",
      "range 0.01–1). The reconstruction error attained by\n",
      "the weighted approach is generally over twenty times\n",
      "smaller than the error of the unweighted solution. Fig-\n",
      "ure 2(b) shows this improvement in the reconstruction\n",
      "error, in terms of the error ratio between the weighted\n",
      "and unweighted solutions, for the data in Figure 2(a),\n",
      "as well as for smaller noise spread ratios of ten and two.\n",
      "Even when the noise variances (and hence the weights)\n",
      "are within a factor of two, we still see a consistent ten\n",
      "percent improvement in reconstruction.\n",
      "\n",
      "The weighted low-rank approximations in this experi-\n",
      "ment were computed using the EM algorithm of Sec-\n",
      "tion 2.2. For a wide noise spread, when the low-\n",
      "rank matrix becomes virtually undetectable (a signal-\n",
      "to-noise ratio well below one, and reconstruction er-\n",
      "rors in excess of the variance of the signal), EM of-\n",
      "ten converges to a non-global minimum. This results\n",
      "in weighted low-rank approximations with errors far\n",
      "higher than could otherwise be expected, as can be\n",
      "seen in both ﬁgures. In such situations, conjugate gra-\n",
      "dient descent methods proved far superior in ﬁnding\n",
      "the global minimum.\n",
      "\n",
      "3. Low-rank Logistic Regression\n",
      "\n",
      "In certain situations we might like to capture a binary\n",
      "data matrix y ∈ {−1, +1}n×d with a low-rank model.\n",
      "A natural choice in this case is a logistic model param-\n",
      "eterized by a low-rank matrix X ∈ <n×d, such that\n",
      "Pr (Yia = +1|Xia) = g(Xia) independently for each\n",
      "i, a, where g is the logistic function g(x) = 1\n",
      "1+e−x . One\n",
      "then seeks a low-rank matrix X maximizing the like-\n",
      "lihood Pr (Y = y|X). Such low-rank logistic models\n",
      "were suggested by Collins et al. (2002) and by Gordon\n",
      "\n",
      "10−110010110210−2100102Signal/Noiseweighted sum2 low rank signal/weighted sum2 noiseReconst. Err: normalized sum2 diff to plantedunweightedweighted10−110010110210300.20.40.60.81Signal/Noiseweighted sum2 low rank sig/weighted sum2 noiseReconst error ratio:weighted / unweightednoise spread=2noise spread=10noise spread=100\f(2003) and recently studied by Schein et al. (2003).\n",
      "\n",
      "Using a weighted low-rank approximation, we can ﬁt\n",
      "a low-rank matrix X minimizing a quadratic loss from\n",
      "the target. In order to ﬁt a non-quadratic loss such as\n",
      "a logistic loss, Loss(Xia; yia) = log g(yiaXia), we use a\n",
      "quadratic approximation to the loss.\n",
      "\n",
      "Consider\n",
      "log g(yx) about ˜x:\n",
      "\n",
      "the\n",
      "\n",
      "second-order Taylor\n",
      "\n",
      "expansion of\n",
      "\n",
      "log g(yx) ≈\n",
      "≈ log g(y˜x) + yg(−y˜x)(x − ˜x) − g(y ˜x)g(−y ˜x)\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)(cid:17)2\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "2\n",
      "\n",
      "≈ − g(y ˜x)g(−y ˜x)\n",
      "\n",
      "x −\n",
      "\n",
      "2\n",
      "\n",
      "˜x + y\n",
      "\n",
      "g(y ˜x)\n",
      "\n",
      "+log g(y˜x)+ g(−y ˜x)\n",
      "2g(y ˜x) .\n",
      "\n",
      "(x − ˜x)2\n",
      "\n",
      "The log-likelihood of a low-rank parameter matrix X\n",
      "can then be approximated as:\n",
      "\n",
      "log Pr (y|X) ≈\n",
      "\n",
      "X\n",
      "\n",
      "−\n",
      "\n",
      "g(yia ˜Xia)g(−yia ˜Xia)\n",
      "2\n",
      "\n",
      "ia\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "Xia −\n",
      "\n",
      "(cid:16) ˜Xia + yia\n",
      "\n",
      "g(yia ˜Xia)\n",
      "\n",
      "(cid:17)(cid:17)2\n",
      "\n",
      "+ Const\n",
      "\n",
      "(4)\n",
      "\n",
      "Maximizing (4) is a weighted low-rank approximation\n",
      "problem. Note that for each entry (i, a), we use a\n",
      "second-order expansion about a diﬀerent point ˜Xia.\n",
      "The closer the origin ˜Xia is to Xia, the better the\n",
      "approximation. This suggests an iterative approach,\n",
      "where in each iteration we ﬁnd a parameter matrix X\n",
      "using an approximation of the log-likelihood about the\n",
      "parameter matrix found in the previous iteration.\n",
      "\n",
      "For the Taylor expansion, the improvement of the\n",
      "approximation is not always monotonic. This might\n",
      "cause the method outlined above not to converge. In\n",
      "order to provide for a more robust method, we use the\n",
      "following variational bound on the logistic (Jaakkola\n",
      "& Jordan, 2000):\n",
      "\n",
      "log g(yx) ≥ log g(y˜x) + yx−y ˜x\n",
      "\n",
      "= − 1\n",
      "4\n",
      "\n",
      "tanh(˜x/2)\n",
      "˜x\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "2 − tanh(˜x/2)\n",
      "(cid:17)\n",
      "x −\n",
      "\n",
      "y ˜x\n",
      "tanh(˜x/2)\n",
      "\n",
      "4˜x\n",
      "\n",
      "(cid:0)x2 − ˜x2(cid:1)\n",
      "\n",
      "+ Const,\n",
      "\n",
      "yielding the corresponding bound on the likelihood:\n",
      "\n",
      "log Pr (y|X) ≥\n",
      "\n",
      "− 1\n",
      "4\n",
      "\n",
      "X\n",
      "\n",
      "ia\n",
      "\n",
      "tanh( ˜Xia/2)\n",
      "˜Xia\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "Xia − yia ˜Xia\n",
      "\n",
      "tanh( ˜Xia/2)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "+ Const\n",
      "\n",
      "(5)\n",
      "\n",
      "with equality if and only if X = ˜X. This bound sug-\n",
      "gests an iterative update of the parameter matrix X (t)\n",
      "by seeking a low-rank approximation X (t+1) for the\n",
      "following target and weight matrices:\n",
      "\n",
      "Fortunately, we do not need to confront the severe\n",
      "problems associated with nesting iterative optimiza-\n",
      "tion methods.\n",
      "In order to increase the likelihood of\n",
      "our logistic model, we do not need to ﬁnd a low-\n",
      "rank matrix minimizing the objective speciﬁed by (6),\n",
      "just one improving it. Any low-rank matrix X (t+1)\n",
      "with a lower objective value than X (t) (with respect\n",
      "to A(t+1) and W (t+1)) is guaranteed to have a higher\n",
      "likelihood: A lower objective corresponds to a higher\n",
      "upper bound in (5), and since the bound is tight for\n",
      "X (t), the log-likelihood of X (t+1) must be higher than\n",
      "the log-likelihood of X (t). Moreover, if the likelihood\n",
      "of X (t) is not already maximal, there are guaranteed\n",
      "to be matrices with lower objective values. Therefore,\n",
      "we can mix weighted low-rank approximation itera-\n",
      "tions and logistic bound update iterations, while still\n",
      "ensuring convergence.\n",
      "\n",
      "In many applications we may also want to associate\n",
      "external weights with each entry in the matrix (e.g.\n",
      "to accommodate missing values), or more generally,\n",
      "weights (counts) of positive and negative observations\n",
      "in each entry (e.g. to capture the likelihood with re-\n",
      "spect to an empirical distribution). This can easily be\n",
      "done by multiplying the weights in (6) by the external\n",
      "weights, or taking a weighted combination correspond-\n",
      "ing to y = +1 and y = −1.\n",
      "\n",
      "Note that the target and weight matrices correspond-\n",
      "ing to the Taylor approximation and those correspond-\n",
      "ing to the variational bound are diﬀerent: The varia-\n",
      "tional target is always closer to the current value of\n",
      "X, and the weights are more subtle. This ensures\n",
      "the guaranteed convergence (as discussed above), but\n",
      "the price we pay is a much lower convergence rate.\n",
      "Although we have observed many instances in which\n",
      "a ‘Taylor’ iteration increases, rather then decreases,\n",
      "the objective, overall convergence was attained much\n",
      "faster using ‘Taylor’, rather than ‘variational’ itera-\n",
      "tions.\n",
      "\n",
      "4. A Collaborative Filtering Example\n",
      "\n",
      "To illustrate the use of weighted, and generalized, low-\n",
      "rank approximations, we applied our methods to a col-\n",
      "laborative ﬁltering problem. The task of collaborative\n",
      "ﬁltering is, given some entries of a user preferences\n",
      "matrix, to predict the remaining entries. We do this\n",
      "by approximating those observed values by a low-rank\n",
      "matrix (using weighted low-rank approximation with\n",
      "zero/one weights). Unobserved values are predicted\n",
      "according to the learned low-rank matrix.\n",
      "\n",
      "A(t+1)\n",
      "W (t+1)\n",
      "ia\n",
      "\n",
      "ia = yia/W (t+1)\n",
      "= tanh(X (t)\n",
      "\n",
      "ia\n",
      "\n",
      "ia /2)/X (t)\n",
      "\n",
      "ia\n",
      "\n",
      "Using low-rank approximation for collaborative ﬁl-\n",
      "tering has been suggested in the past. Goldberg\n",
      "\n",
      "(6)\n",
      "\n",
      "\fet al. (2001) use a low-rank approximation of a fully-\n",
      "observed subset of columns of the matrix, thus avoid-\n",
      "ing the need to introduce weights. Billsus and Paz-\n",
      "zani (1998) use a singular value decomposition of a\n",
      "sparse binary observation matrix. Both Goldberg and\n",
      "Billsus use the low-rank approximation only as a pre-\n",
      "processing step, and then use clustering (Goldberg)\n",
      "and neural networks (Billsus) to learn the preferences.\n",
      "Azar et al. (2001) proved asymptotic consistency of\n",
      "a method in which unobserved entries are replaced by\n",
      "zeros, and observed entries are scaled inversely pro-\n",
      "portionally to the probability of them being observed.\n",
      "No guarantees are provided for ﬁnite data sets, and to\n",
      "the best of our knowledge this technique has not been\n",
      "experimentally tested.\n",
      "\n",
      "We analyzed a subset of the Jester data5 (Goldberg\n",
      "et al., 2001). The data set contains one hundred jokes,\n",
      "with user ratings (bounded continuous values entered\n",
      "by clicking an on-screen “funniness” bar) for some of\n",
      "the jokes. All users rated a core set of ten jokes, and\n",
      "most users rated an extended core set of a total of\n",
      "twenty jokes. Each user also rated a variable number of\n",
      "additional jokes. We selected at random one thousand\n",
      "users who rated the extended core set and at least two\n",
      "additional jokes. For each user, we selected at random\n",
      "two non-core jokes and held out their ratings. We ﬁt\n",
      "low-rank matrices using the following techniques:\n",
      "svd Unobserved values were replaced with zeros, and\n",
      "the unweighted low-rank approximation to the re-\n",
      "sulting matrix was sought.\n",
      "\n",
      "subset An unweighted low-rank approximation for\n",
      "the core subset of jokes was sought (similarly to\n",
      "Goldberg’s initial step). The matrix was extended\n",
      "to the remaining jokes by projecting each joke col-\n",
      "umn onto the column subspace of this matrix.\n",
      "\n",
      "rescaling Following Azar et al.\n",
      "\n",
      "(2001), the ratings\n",
      "for each joke were scaled inversely proportional\n",
      "to the frequency with which the joke was rated\n",
      "(between 0.197 and 0.77). An unweighted low-\n",
      "rank approximation for the resulting matrix was\n",
      "sought.\n",
      "\n",
      "wlra A weight of one was assigned to each observed\n",
      "joke, and a weight of zero to each unobserved\n",
      "joke, and a weighted low-rank approximation was\n",
      "sought using gradient descent techniques.\n",
      "\n",
      "For each low-rank matrix, the test error on the held out\n",
      "jokes (Figure 3) and the training error were measured\n",
      "in terms of the average squared diﬀerence to the true\n",
      "rating, scaled by the possible range of ratings. Normal-\n",
      "ized mean absolute error (NMAE) was also measured,\n",
      "producing very similar results, with no qualitative dif-\n",
      "\n",
      "5The data set was kindly provided by Ken Goldberg.\n",
      "\n",
      "Figure 3. Prediction errors on Jester jokes: test error (main\n",
      "ﬁgure) and training error (insert).\n",
      "\n",
      "ferences. Beyond the consistent reduction in training\n",
      "error (which is guaranteed by the optimization objec-\n",
      "tive), we observe that wlra achieves a better test error\n",
      "than any of the other methods. Not surprisingly, it\n",
      "also over-ﬁts much more quickly, as it becomes possi-\n",
      "ble to approximate the observed values better at the\n",
      "expense of extreme values in the other entries.\n",
      "\n",
      "Figure 4. Training (dotted lines) and test performance on\n",
      "Jester jokes.\n",
      "\n",
      "As discussed in the introduction, minimizing the\n",
      "squared error to the absolute ratings is not necessar-\n",
      "ily the correct objective. Taking the view that each\n",
      "joke has a ‘probability of being funny’ for each user,\n",
      "we proceeded to try to ﬁt a low-rank logistic regres-\n",
      "sion model. We ﬁrst transformed the raw observed\n",
      "values into ‘funniness’ probabilities by ﬁtting a mix-\n",
      "ture model with two equal-variance Gaussian com-\n",
      "ponents to each user’s ratings, and using the result-\n",
      "ing component-posterior probabilities. This procedure\n",
      "\n",
      "02468101214160.140.160.180.20.220.240.260.280.30.32rank of approximationsqrt(normalized average sum square diff)svdsvd on subsetsvd w/ rescalingwlra 0.150.2  training error02468101214160.60.81rank of approximation% order agreements for extreme pairssvd (training)wlrawlra on probslogisticsvd (testing)wlrawlra on probslogistic\falso ensures scale and transformation invariability for\n",
      "a user’s ratings, and places more emphasis on users\n",
      "with a bimodal rating distribution than on users for\n",
      "which all ratings are clustered together. We proceeded\n",
      "to ﬁt a low-rank logistic model (q.v. Section 3) using\n",
      "the observed posterior probabilities as empirical prob-\n",
      "abilities. Since the resulting low-rank model no longer\n",
      "predicts the absolute rating of jokes, we measured suc-\n",
      "cess by analyzing the relative ranking of jokes by each\n",
      "user. Speciﬁcally, for each user we held out one non-\n",
      "core joke which was rated among the top quarter by\n",
      "the user, and one non-core joke which was rated in\n",
      "the bottom quarter. We then measured the frequency\n",
      "with which the relative rankings of the predictions on\n",
      "these two jokes was consistent with the true relative\n",
      "ranking. Using this measure, we compared the logistic\n",
      "low-rank model to the sum-squared error methods dis-\n",
      "cussed above, applied to both the absolute ratings (as\n",
      "above) and the probabilities. Figure 4 shows the train-\n",
      "ing and test performance of the logistic method, the\n",
      "wlra method applied to the ratings, the wlra method\n",
      "applied to the probabilities, and the svd method ap-\n",
      "plied to the ratings (all other methods tested perform\n",
      "worse than those shown). Although the results indi-\n",
      "cate that the wlra method performs better than the\n",
      "logistic method, it is interesting to note that for small\n",
      "ranks, k = 2, 3, the training performance of the lo-\n",
      "gistic model is better—in these cases the logistic view\n",
      "allows us to better capture the rankings than a sum-\n",
      "squared-error view (Schein et al. (2003) investigates\n",
      "the training error of other data sets, and arrives at\n",
      "similar conclusions). A possible modiﬁcation to the\n",
      "logistic model that might make it more suitable for\n",
      "such tasks is the introduction of label noise.\n",
      "\n",
      "5. Conclusion\n",
      "\n",
      "We have provided simple and eﬃcient algorithms for\n",
      "solving weighted low-rank approximation problems.\n",
      "The EM algorithm is extremely simple to implement,\n",
      "and works well in some cases. In more complex cases,\n",
      "conjugate gradient descent on J ∗(V ) provides eﬃcient\n",
      "convergence, usually to the global minimum.\n",
      "\n",
      "Weighted low-rank approximation problems are im-\n",
      "portant in their own right and appear as subroutines in\n",
      "solving a class of more general low-rank problems. One\n",
      "such problem, ﬁtting a low-rank logistic model, was de-\n",
      "veloped in this paper. Similar approaches can be used\n",
      "for other convex loss functions with a bounded Hes-\n",
      "sian. Another class of problems that we can solve us-\n",
      "ing weighted low-rank approximation as a subroutine\n",
      "is low-rank approximation with respect to a mixture-\n",
      "of-Gaussians noise model. This application will be\n",
      "\n",
      "treated in depth in a separate paper.\n",
      "\n",
      "References\n",
      "\n",
      "Azar, Y., Fiat, A., Karlin, A. R., McSherry, F., & Saia,\n",
      "J. (2001). Spectral analysis of data. Proceedings\n",
      "of the Thirty Third ACM Symposium on Theory of\n",
      "Computing.\n",
      "\n",
      "Billsus, D., & Pazzani, M. J. (1998). Learning col-\n",
      "laborative information ﬁlters. Proceedings of 15th\n",
      "International Conference on Machine Learning.\n",
      "\n",
      "Collins, M., Dasgupta, S., & Schapire, R. (2002). A\n",
      "generalization of principal component analysis to\n",
      "the exponential family. Advances in Neural Infor-\n",
      "mation Processing Systems 14.\n",
      "\n",
      "Goldberg, K., Roeder, T., Gupta, D., & Perkins, C.\n",
      "(2001). Eigentaste: A constant time collaborative\n",
      "ﬁltering algorithm. Information Retrieval, 4, 133–\n",
      "151.\n",
      "\n",
      "Gordon, G. (2003). Generalized2 linear2 models. Ad-\n",
      "vances in Neural Information Processing Systems\n",
      "15.\n",
      "\n",
      "Irani, M., & Anandan, P. (2000). Factorization with\n",
      "uncertainty. Proceedings of the Sixth European Con-\n",
      "ference on Computer Vision.\n",
      "\n",
      "Jaakkola, T., & Jordan, M. (2000). Bayesian param-\n",
      "eter estimation via variational methods. Statistics\n",
      "and Computing, 10, 25–37.\n",
      "\n",
      "Lu, W.-S., Pei, S.-C., & Wang, P.-H. (1997). Weighted\n",
      "low-rank approximation of general complex matrices\n",
      "and its application in the design of 2-D digital ﬁlters.\n",
      "IEEE Transactions on Circuits and Systems—I, 44,\n",
      "650–655.\n",
      "\n",
      "Schein, A. I., Saul, L. K., & Ungar, L. H. (2003).\n",
      "A generalized linear model for principal component\n",
      "analysis of binary data. Proceedings of the Ninth In-\n",
      "ternational Workshop on Artiﬁcial Intelligence and\n",
      "Statistics.\n",
      "\n",
      "Shpak, D. (1990). A weighted-least-squares matrix de-\n",
      "composition method with application to the design\n",
      "of two-dimensional digital ﬁlters. IEEE Thirty Third\n",
      "Midwest Symposium on Circuits and Systems.\n",
      "\n",
      "Tenenbaum, J. B., & Freeman, W. T. (2000). Separat-\n",
      "ing style and content with bilinear models. Neural\n",
      "Computation, 12, 1247–1283.\n",
      "\n",
      "Young, G. (1940). Maximum likelihood estimation and\n",
      "\n",
      "factor analysis. Psychometrika, 6, 49–53.\n",
      "\n",
      " model output {\"references\": [\n",
      "        {\n",
      "            \"title\": \"Azar, Y., Fiat, A., Karlin, A. R., McSherry, F., & Saia, J. (2001). Spectral analysis of data.\",\n",
      "            \"authors\": [\"Y. Azar\", \"A. Fiat\", \"A. R. Karlin\", \"F. McSherry\", \"J. Saia\"],\n",
      "            \"date\": \"2001\",\n",
      "            \"journal\": null\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Billsus, D., & Pazzani, M. J. (1998). Learning col-laborative information ﬁlters.\",\n",
      "            \"authors\": [\"D. Billsus\", \"M. J. Pazzani\"],\n",
      "            \"date\": \"1998\",\n",
      "            \"journal\": null\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Collins, M., Dasgupta, S., & Schapire, R. (2002). A generalization of principal component analysis to the exponential family.\",\n",
      "            \"authors\": [\"M. Collins\", \"S. Dasgupta\", \"R. Schapire\"],\n",
      "            \"date\": \"2002\",\n",
      "            \"journal\": null\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Goldberg, K., Roeder, T., Gupta, D., & Perkins, C. (2001). Eigentaste: A constant time collaborative ﬁltering algorithm.\",\n",
      "            \"authors\": [\"K. Goldberg\", \"T. Roeder\", \"D. Gupta\", \"C. Perkins\"],\n",
      "            \"date\": \"2001\",\n",
      "            \"journal\": null\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Gordon, G. (2003). Generalized linear models for principal component analysis of binary data.\",\n",
      "            \"authors\": [\"G. Gordon\"], \"date\": \"2003\", \"journal\": null\n",
      "        }\n",
      "    ]}\n",
      "    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "titles_count 5\n",
      "therein]  as  well  as  in\n",
      "the NN  literature (Rumelhart, 1988; Chauvin, 1989;  Hanson and Pratt, 1989; Mozer\n",
      "and  Smolensky,  1989).\n",
      "\n",
      "Various  complexity  measures  have  been  proposed,  including  Vapnik-Chervonenkis\n",
      "dimensionality (Vapnik and  Chervonenkis,  1971)  and description  length (Rissanen,\n",
      "1989) .  A time-honored (albeit inexact) measure of complexity is simply the number\n",
      "of non-zero  free  parameters,  which  is  the  measure  we  choose  to  use  in  this  paper\n",
      "[but  see  (Denker,  Le  Cun and  Solla,  1990)].  Free  parameters  are  used  rather  than\n",
      "connections,  since in constrained networks,  several connections can be controlled by\n",
      "a  single parameter.\n",
      "\n",
      "In most cases in the statistical inference literature, there is  some a priori or heuristic\n",
      "information  that  dictates  the  order  in  which  parameters  should  be  deleted;  for\n",
      "example,  in a family of polynomials, a  smoothness heuristic  may require high-order\n",
      "terms  to  be  deleted  first.  In  a  neural  network,  however,  it  is  not  at  all obvious in\n",
      "which order the  parameters should  be deleted.\n",
      "\n",
      "A  simple strategy  consists  in deleting parameters with small  \"saliency\",  i.e.  those\n",
      "whose  deletion  will  have  the  least  effect  on  the  training error.  Other  things  be\n",
      "ing equal,  small-magnitude parameters will have  the least saliency,  so a  reasonable\n",
      "initial strategy  is  to  train  the  network  and  delete  small-magnitude parameters in\n",
      "order.  After  deletion,  the  network  should  be  retrained.  Of course  this  procedure\n",
      "can  be iterated;  in the limit it  reduces  to  continuous weight-decay during  training\n",
      "(using disproportionately rapid decay of small-magnitude parameters).  In fact, sev\n",
      "eral network minimization schemes have been implemented using  non-proportional\n",
      "weight decay  (Rumelhart,  1988;  Chauvin, 1989;  Hanson and  Pratt,  1989), or \"gat\n",
      "ing  coefficients\"  (Mozer  and  Smolensky,  1989).  Generalization  performance  has\n",
      "been  reported  to increase significantly on the somewhat small problems  examined.\n",
      "Two drawbacks of these  techniques are that they  require  fine-tuning of the  \"prun\n",
      "ing\"  coefficients  to  avoid  catastrophic  effects,  and  also  that  the  learning  process\n",
      "is  significantly  slowed  down.  Such  methods  include  the  implicit  hypothesis  that\n",
      "the  appropriate  measure  of network  complexity  is  the  number  of parameters  (or\n",
      "sometimes the number of units)  in  the network.\n",
      "\n",
      "One  of the  main  points of this  paper  is  to  move  beyond  the  approximation  that\n",
      "\"magnitude equals saliency\" , and propose a  theoretically justified saliency measure.\n",
      "\n",
      "\f600\n",
      "\n",
      "Le Cun, Denker and Solla\n",
      "\n",
      "Our  technique  uses  the  second  derivative of the objective function  with  respect  to\n",
      "the  parameters  to  compute  the  saliencies.  The  method  was  ,,-alidated  using  our\n",
      "handwritten digit recognition network trained with backpropagation (Le Cun et aI.,\n",
      "1990b).\n",
      "\n",
      "2  OPTIMAL  BRAIN DAMAGE\n",
      "\n",
      "Objective  functions  playa central  role  in  this  field;  therefore  it  is  more  than  rea\n",
      "sonable  to  define  the  saliency  of a  parameter  to  be  the  change  in  the  objective\n",
      "function caused  by deleting that parameter.  It would  be prohibiti,-ely laborious to\n",
      "evaluate the saliency directly from  this definition,  i.e.  by  temporarily deleting each\n",
      "parameter and  reevaluating  the objective function.\n",
      "\n",
      "Fortunately,  it  is  possible  to  construct  a  local  model  of  the  error  function  and\n",
      "analytically  predict the effect  of perturbing the  parameter  vector.  \"'e approximate\n",
      "the  objective  function  E  by  a  Taylor series.  A  perturbation  lL~  of the  parameter\n",
      "vector  will change the objective function  by\n",
      "\n",
      "(1)\n",
      "\n",
      "Here,  the 6ui'S are the components of flJ,  the gi's are the components of the gradient\n",
      "G  of E  with  respect  to U,  and  the  hi;'S are the elements  of the  Hessian  matrix H\n",
      "of E  with  respect  to U:\n",
      "\n",
      "8E\n",
      "gi=  -8\n",
      "Ui\n",
      "\n",
      "and\n",
      "\n",
      "(2)\n",
      "\n",
      "The goal is  to find  a  set of parameters whose deletion  will cause  the least  increase\n",
      "of E .  This  problem  is  practically  insoluble  in  the  general  case.  One  reason  is\n",
      "that the matrix H  is  enormous (6.5 x  106  terms for our 2600  parameter  network),\n",
      "and  is  very  difficult  to  compute.  Therefore  we  must  introduce  some  simplifying\n",
      "approximations.  The  \"diagonal\"  approximation  assumes  that  the  6E  caused  by\n",
      "deleting several parameters is the sum of the 6E's caused by delet~ each parameter\n",
      "individually;  cross  terms  are  neglected,  so  third  term  of  the  npt hand  side  of\n",
      "equation  1  is  discarded.  The  \"extremal\"  approximation  assumes  that  parameter\n",
      "deletion  will  be  performed  after  training  has  converged.  The parameter  vector  is\n",
      "then at a (local) minimum of E and the first term of the right hand side of equation 1\n",
      "can  be neglected.  Furthermore,  at a  local minimum,  all the hii's are non-negative,\n",
      "so  any  perturbation  of the  parameters  will  cause  E  to increase  or stay  the  same.\n",
      "Thirdly,  the  \"quadratic\"  approximation  assumes  that  the  cost  fundion  is  nearly\n",
      "quadratic 80  that the last term in  the equation  can be  neglected.  Equation  1 then\n",
      "reduces  to\n",
      "\n",
      "6E=~~h\"6u~\n",
      "2L.i  \"\n",
      "•\n",
      "i\n",
      "\n",
      "(3)\n",
      "\n",
      "\fOptimal Brain Damage\n",
      "\n",
      "601\n",
      "\n",
      "2.1  COMPUTING  THE  SECOND  DERIVATIVES\n",
      "\n",
      "Now  we  need  an  efficient  way  of computing  the  diagonal  second  derivatives  hii .\n",
      "Such  a  procedure  was derived  in (Le Cun,  1987),  and  was  the basis of a  fast  back\n",
      "propagation method  used  extensively  in  \\1lrious applications (Becker  and  Le  Cun,\n",
      "1989;  Le  Cun,  1989;  Le  Cun  et  al.,  1990a).  The  procedure  is  very  similar  to  the\n",
      "back-propagation algorithm used  for  computing  the first  derivatives.  We  will  only\n",
      "outline the  proced ure;  details can be found  in the references.\n",
      "\n",
      "We  assume  the objective function  is the  usual  mean-squared  error (MSE);  general\n",
      "ization to other  additive error  measures  is  straightforward.  The following  expres\n",
      "sions  apply  to  a  single  input  pattern;  afterward  E  and  H  must  be  averaged  over\n",
      "the training set.  The network  state is  computed  using  the standard formulae\n",
      "\n",
      "and\n",
      "\n",
      "ai = L WijZj\n",
      "\n",
      "j\n",
      "\n",
      "( 4)\n",
      "\n",
      "where  Zi  is  the state of unit  i,  ai  its  total  input  (weighted  sum),  !  the  squashing\n",
      "function  and  Wij  is  the connection going  from unit j  to unit  i.  In  a  shared-weight\n",
      "network like ours, a single parameter Uk  can control one or more connections:  Wij  =\n",
      "Uk  for  all (i, j) E Vk,  where Vk  is a set of index pairs.  By the chain rule,  the diagonal\n",
      "terms  of H  are  given  by\n",
      "\n",
      "hu =  L  {)w~,\n",
      "\n",
      "{)2E\n",
      "\n",
      "(i,j)EV.\n",
      "\n",
      ".,\n",
      "\n",
      "The summand  can  be expanded  (using the basic  network equations 4)  as:\n",
      "\n",
      "lP E\n",
      "{J2E\n",
      "- -= -z ·\n",
      "{Ja~'\n",
      "{Jw~.\n",
      "\n",
      "2\n",
      "\n",
      ".\n",
      "\n",
      ".,\n",
      "\n",
      "The second  derivatives are back-propagated from layer to layer:\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "(7)\n",
      "\n",
      "We  also  need  the  boundary  condition  at  the  output  layer,  specifying  the  second\n",
      "derivative of E  with respect  to the last-layer weighted  BUms:\n",
      "\n",
      "{J{J2 ~ = 2!'(ai)2 - 2(di - Zi)!\"(ai)\n",
      "ai\n",
      "\n",
      "(8)\n",
      "\n",
      "for all units i  in  the output layer.\n",
      "\n",
      "As can be seen,  computing the diagonal Hessian is of the same order of complexity\n",
      "as computing the gradient.  In some cases,  the second  term of the right hand side of\n",
      "the last two equations (involving the second derivative of I) can be neglected.  This\n",
      "corresponds  to  the  well-known  Levenberg-Marquardt  approximation,  and  has  the\n",
      "interesting property of giving guaranteed positive estimates of the second derivative.\n",
      "\n",
      "\f602\n",
      "\n",
      "Le Cun, Denker and Solla\n",
      "\n",
      "2.2  THE  RECIPE\n",
      "\n",
      "The OBO  procedure can  be  carried  out as follows:\n",
      "\n",
      "1.  Choose a  reasonable network  architecture\n",
      "2.  Train the network  until a  reasonable solution is obtained\n",
      "3.  Compute the second  derivatives hu for each  parameter\n",
      "4.  Compute the saliencies for  each parameter:  Sk  =  huu~/2\n",
      "5.  Sort  the parameters by saliency  and delete some low-saliency  parameters\n",
      "6.  Iterate to step 2\n",
      "\n",
      "Deleting  a  parameter  is  defined  as  setting  it  to 0  and  freezing  it  there.  Several\n",
      "variants of the procedure  can be devised,  such  as decreasing  the ... 41ues  of the low\n",
      "saliency  parameters  instead  of simply  setting  them  to  0,  or  allowing  the  deleted\n",
      "parameters to adapt again after they  have  been set  to o.\n",
      "\n",
      "2.3  EXPERIMENTS\n",
      "\n",
      "The simulation results  given  in  this section  were  obtained  using  back-propagation\n",
      "applied to handwritten digit recognition.  The initial network was highly constrained\n",
      "and sparsely connected,  having 105  connections controlled by 2578  free  parameters.\n",
      "It  was  trained  on a  database of segmented  handwritten  zip code digits and  printed\n",
      "digits  containing  approximately  9300  training examples  and  3350  t.est  examples.\n",
      "More details can  be obtained from  the companion paper (Le Cun et al.,  1990b).\n",
      "\n",
      "16\n",
      "14\n",
      "1\n",
      "10\n",
      "pJ  8\n",
      "~6\n",
      "b04\n",
      "\n",
      "o -\n",
      "\n",
      "<a>\n",
      "\n",
      "Magnitude\n",
      "\n",
      "16\n",
      "14\n",
      "1\n",
      "10\n",
      "pJ  8\n",
      "~6\n",
      "b04\n",
      ".9\n",
      "\n",
      "OBD\n",
      "\n",
      "(b)\n",
      "\n",
      "o\n",
      "~~--~--~---+--~----~\n",
      "500  1000  1500  2000  2SOO\n",
      "\n",
      "o\n",
      "\n",
      "Parameters\n",
      "\n",
      "o\n",
      "-2~ __ ~ __ ~ __ -+ ________ ~\n",
      "laX)  2SOO\n",
      "\n",
      "o\n",
      "\n",
      "SOO  1000  1500\n",
      "Parameters\n",
      "\n",
      "Figure 1:  (a)  Objective  function  (in  dB)  versus  number  of paramet.ers  for  OBn\n",
      "(lower curve) and magnitude-based parameter deletion (upper curve).  (b) Predicted\n",
      "and  actual  objective  function  versus  number  of parameters.  The  predicted  value\n",
      "(lower curve)  is the sum of the saliencies of the deleted  parameters.\n",
      "\n",
      "Figure  la shows  how  the  objective  function  increases  (from  right  to left)  as  the\n",
      "number of remaining parameters decreases.  It is clear that  deletin~ parameters by\n",
      "\n",
      "\fOptimal Brain Damage\n",
      "\n",
      "603\n",
      "\n",
      "order of saliency causes a significantly smaller increase of the objective function than\n",
      "deleting them according to their magnitude.  Random deletions were  also tested  for\n",
      "the sake of comparison,  but the performance was so bad  that the curves  cannot be\n",
      "shown on  the same scale.\n",
      "\n",
      "Figure 1b shows how the objective function increases (from right to left) as the num\n",
      "ber  of remaining  parameters decreases,  compared  to the increase  predicted  by  the\n",
      "Quadratic-Extremum-Diagonal approximation.  Good  agrement  is  obtained  for  up\n",
      "to approximately 800  deleted  parameters  (approximately  30%  of the  parameters).\n",
      "Beyond  that  point,  the  curves  begin  to split,  for  several  reasons:  the  off-diagonal\n",
      "terms  in  equation  1  become  disproportionately  more  important  as  the  number of\n",
      "deleted  parameters increases,  and higher-than-quadratic terms become more impor-\n",
      "tant when  larger-valued parameters are deleted.\n",
      "\n",
      "'\n",
      "\n",
      "16\n",
      "14\n",
      "1\n",
      "\n",
      "10\n",
      "\n",
      "<a)\n",
      "\n",
      "16\n",
      "14\n",
      "1\n",
      "\n",
      "10\n",
      "\n",
      "(b)\n",
      "\n",
      "UJ  8\n",
      "~ 6\n",
      "~4\n",
      "\n",
      "o -\n",
      "\n",
      "o\n",
      "-2~--4----+----~--~--~\n",
      "1000  1500  2000  2500\n",
      "\n",
      "SOO\n",
      "\n",
      "o\n",
      "\n",
      "UJ  8\n",
      "~ 6\n",
      "~ 4.\n",
      "~~\n",
      "-2~I--~,~ __ +, ____ ~, __ ~I ____ ~I\n",
      "\n",
      "500  1000  1500  2000  2500\n",
      "\n",
      "o\n",
      "\n",
      "Parameters\n",
      "\n",
      "Parameters\n",
      "\n",
      "Figure 2:  Objective  function  (in  dB)  versus  number  of parameters,  without  re\n",
      "training (upper curve),  and after  retraining (lower curve).  Curves are given for  the\n",
      "training set (a)  and the  test set  (b).\n",
      "\n",
      "Figure 2 shows  the log-MSE on the training set  and  the on  the test  set  before  and\n",
      "after  retraining.  The  performance  on  the  training set  and  on  the  test  set  (after\n",
      "retraining)  stays almost  the same  when  up to 1500  parameters  (60%  of the  total)\n",
      "are deleted.\n",
      "\n",
      "We  have  also  used  OBn  as  an  interactive  tool  for  network  design  and  analysis.\n",
      "This contrasts  with  the  usual  view  of weight  deletion  as a  more-or-Iess  automatic\n",
      "procedure.  Specifically,  we  prepared  charts  depicting  the  saliency  of the  10,000\n",
      "parameters in the digit recognition network reported last year (Le Cun et aI., 1990b).\n",
      "To  our  surprise,  several  large  groups  of parameters  were  expendable.  We  were\n",
      "able to excise  the second-to-Iast  layer,  thereby  reducing  the  number of parameters\n",
      "by  a  factor  of  two.  The  training  set  MSE  increased  by  a  factor  of 10,  and  the\n",
      "generalization  MSE  increased  by  only  50%.  The  10-category  classification  error\n",
      "on  the  test  set  actually  decreased  (which  indicates  that  MSE  is  not  the  optimal\n",
      "\n",
      "\f604\n",
      "\n",
      "Le Cun, Denker and Solla\n",
      "\n",
      "objective  function  for  this  task).  OBD  motivated  other  architectural  changes,  as\n",
      "can be seen  by comparing the 2600-parameter network in (Le Cun et aI.,  1990a)  to\n",
      "the 1O,OOO-parameter  network in (Le Cun et aI.,  1990b).\n",
      "\n",
      "3  CONCLUSIONS  AND  OUTLOOK\n",
      "\n",
      "We  have used  Optimal Brain Damage interactively to reduce  the number of param\n",
      "eters  in  a  practical  neural  network  by  a  factor of four.  We obtained  an  additional\n",
      "factor of more than two by using OBD to delete parameters automatically.  The net\n",
      "work's speed  improved significantly, and  its recognition accuracy  increased  slightly.\n",
      "We  emphasize  that  the  starting  point  was  a  state-of-the-art  network.  It would  be\n",
      "too easy to start with a  foolish  network  and make large improvements:  a  technique\n",
      "that can help  improve an already-good  network  is  particularly valuable.\n",
      "\n",
      "We believe  that the techniques  presented  here only scratch  the surface of the appli\n",
      "cations where  second-derivative information can and should be used.  In particular,\n",
      "we  have also been able to move beyond  the approximation that \"complexity equals\n",
      "number of free  parameters\"  by using second-derivative information.  In (Denker,  Le\n",
      "Cun and Solla,  1990),  we  use  it  to to derive  an improved  measure of the network's\n",
      "information  content,  or  complexity.  This  allows  us  to compare  network  architec\n",
      "tures on a  given  task,  and  makes  contact  with the notion of Minimum  Description\n",
      "Length  (MDL)  (Rissanen,  1989).  The main idea is  that a  \"simple\"  network  whose\n",
      "description  needs  a  small number of bits is  more likely  to generalize correctly  than\n",
      "a  more  complex  network,  because  it  presumably  has  extracted  the  essence  of the\n",
      "data and removed  the  redundancy from  it.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "We  thank  the  US  Postal  Service  and  its  contractors  for  providing  us  with  the\n",
      "database.  We also thank Rich Howard  and Larry Jackel for  their helpful comments\n",
      "and encouragements.  We especially thank David Rumelhart for sharing unpublished\n",
      "ideas.\n",
      "\n",
      "References\n",
      "\n",
      "Akaike, H. (1986).  Use of Statistical Models for Time Series Analysis.  In Proceedings\n",
      "\n",
      "ICASSP 86,  pages 3147-3155, Tokyo.  IEEE.\n",
      "\n",
      "Baum,  E.  B.  and  Haussler,  D.  (1989).  What Size  Net  Gives  Valid  Generaliztion?\n",
      "\n",
      "Neural  Computation,  1:151-160.\n",
      "\n",
      "Becker,  S.  and Le Cun, Y.  (1989).  Improving the Convergence of Back-Propagation\n",
      "Learning with Second-Order  Methods.  In Touretzky,  D.,  Hinton,  G., and  Se\n",
      "jnowski,  T., editors,  Proc.  of the  1988  Connectionist  Model&  S.mmer School,\n",
      "pages 29-37, San Mateo.  Morgan Kaufman.\n",
      "\n",
      "Chauvin,  Y.  (1989).  A  Back-Propagation  Algorithm  with  Optimal  Use  of Hid\n",
      "den  Units.  In  Touretzky,  D.,  editor,  Neural  Information  Proce$$ing  S,&tems,\n",
      "volume 1,  Denver,  1988.  Morgan Kaufmann.\n",
      "\n",
      "\fOptimal Brain Damage\n",
      "\n",
      "605\n",
      "\n",
      "Denker,  J.,  Schwartz,  D.,  Wittner,  B.,  Solla,  S.  A.,  Howard,  R.,  Jackel,  L.,  and\n",
      "Hopfield,  J.  (1987).  Large  Automatic Learning,  Rule Extraction and  General\n",
      "ization.  Complex  Systems,  1:877-922.\n",
      "\n",
      "Denker,  J.  S.,  Le  Cun,  Y.,  and  Solla,  S.  A.  (1990).  Optimal Brain  Damage.  To\n",
      "\n",
      "appear in Computer and  System Sciences.\n",
      "\n",
      "Hanson,  S.  J.  and  Pratt,  L.  Y.  (1989).  Some Comparisons of Constraints for  Min\n",
      "imal  Network  Construction  with  Back-Propagation.  In Touretzky,  D.,  editor,\n",
      "Neural Information  Processing Systems,  volume 1, Denver,  1988. Morgan Kauf\n",
      "mann.\n",
      "\n",
      "Le  Cun,  Y.  (1987).  Modeles  Connexionnistes  de  l'Apprentissage.  PhD  thesis,  Uni\n",
      "\n",
      "versite  Pierre et Marie Curie,  Paris,  France.\n",
      "\n",
      "Le  Cun,  Y.  (1989).  Generalization  and  Network  Design  Strategies.  In  Pfeifer,  R.,\n",
      "Schreter,  Z.,  Fogelman,  F.,  and  Steels,  L.,  editors,  Connectionism  in  Perspec\n",
      "tive,  Zurich,  Switzerland.  Elsevier.\n",
      "\n",
      "Le  Cun,  Y.,  Boser,  B.,  Denker,  J.  S.,  Henderson,  D.,  Howard,  R.  E.,  Hubbard,\n",
      "W.,  and  Jackel,  L.  D.  (1990a) .  Handwritten  Digit  Recognition  with  a  Back\n",
      "Propagation Network.  In Touretzky,  D., editor,  Neural Information  Processing\n",
      "Systems,  volume 2,  Denver,  1989.  Morgan Kaufman.\n",
      "\n",
      "Le  Cun, Y.,  Boser,  B.,  Denker,  J.  S.,  Henderson,  D.,  Howard,  R.  E.,  Hubbard,  W.,\n",
      "and Jackel,  L.  D.  (1990b).  Back-Propagation Applied  to Handwritten Zipcode\n",
      "Recognition.  Neural  Computation,  1{ 4).\n",
      "\n",
      "Mozer,  M.  C.  and  Smolensky,  P.  (1989).  Skeletonization:  A  Technique  for  Trim\n",
      "ming  the  Fat  from  a  Network  via  Relevance  Assessment.  In  Touretzky,  D.,\n",
      "editor,  Neural  Information  Processing  Systefn$,  volume  1,  Denver,  1988.  Mor\n",
      "gan Kaufmann.\n",
      "\n",
      "Rissanen,  J.  (1989).  Stochastic  Complexity  in  Statistical Inquiry.  World Scientific,\n",
      "\n",
      "Singapore.\n",
      "\n",
      "Rumeihart,  D.  E.  (1988).  personal communication.\n",
      "Solla,  S.  A.,  Schwartz,  D.  B.,  Tishby,  N.,  and Levin,  E.  (1990).  Supervised  Learn\n",
      "ing:  a  Theoretical  Framework.  In  Touretzky,  D.,  editor,  Neural  Information\n",
      "Processing  Systems,  volume 2,  Denver,  1989.  Morgan Kaufman.\n",
      "\n",
      "Vapnik, V.  N. (1989).  Inductive Principles of the Search for Empirical Dependences.\n",
      "In Proceedings  of the second annual Workshop  on  Computational Learning The\n",
      "ory,  pages 3-21.  Morgan Kaufmann.\n",
      "\n",
      "Vapnik,  V.  N.  and  Chervonenkis,  A.  Y.  (1971).  On  the  Uniform  Convergence  of\n",
      "\n",
      "Relative Frequencies of Events to Their Probabilities.  Th.  Pro6.  and its Appli\n",
      "cations,  17(2):264-280.\n",
      "\n",
      "Waibel,  A.  (1989).  Consonant  Recognition  by  Modular  Construction  of Large\n",
      "Phonemic  Time-Delay Neural  Networks.  In  Touretzky,  D.,  editor,  Neural In\n",
      "formation  Processing Systems, volume 1,  pages 215-223, Denver,  1988. Morgan\n",
      "Kaufmann.\n",
      "\n",
      " model output {\"references\": [\n",
      "        {\n",
      "            \"title\": \"Vapnik, V. N.\",\n",
      "            \"authors\": [\"Vapnik, V. N.\"],\n",
      "            \"date\": \"1989\",\n",
      "            \"journal\": null\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Mozer, M. C.\",\n",
      "            \"authors\": [\"M. C. Mozer\"],\n",
      "            \"date\": \"1989\",\n",
      "            \"journal\": null\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Vapnik, V. N.\",\n",
      "            \"authors\": [\"Vapnik, V. N.\"],\n",
      "            \"date\": \"1989\",\n",
      "            \"journal\": null\n",
      "        }\n",
      "    ]}\n",
      "    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "titles_count 3\n",
      "Hassibi, B. Stork, D. G. and Wolff, G.  (1993a).  Optimal Brain Surgeon and general network pruning\n",
      "\n",
      "(submitted to ICNN, San Francisco)\n",
      "\n",
      "Hassibi, B. Stork, D. G. and Wolff, G.  (1993b).  Optimal Brain Surgeon, Information Theory and network\n",
      "\n",
      "capacity control (in preparation)\n",
      "\n",
      "Hertz, J., Krogh, A. and Palmer, R. G. (1991). Introduction to the Theory of Neural Computation\n",
      "\n",
      "Addison-Wesley.\n",
      "\n",
      "Kailath, T.  (1980). Linear Systems Prentice-Hall.\n",
      "Kung, S. Y.  and Hu, Y.  H. (1991). A Frobenius approximation reduction method (FARM) for detennining\n",
      "\n",
      "the optimal number of hidden units, Proceedings of the IJCNN-9I Seattle, Washington.\n",
      "\n",
      "Le Cun, Y., Denker, J.  S. and SoUa, S.  A. (1990). Optimal Brain Damage, in Proceedings of the Neural\n",
      "\n",
      "Information Processing Systems-2, D. S. Touretzky (ed.) 598-605, Morgan-Kaufmann.\n",
      "\n",
      "Rissanen, J.  (1978). Modelling by shortest data description, Aulomatica 14,465-471.\n",
      "Rumelhart, D.  E., Hinton, G. E., and Williams, R. J. (1986). Learning Internal representations by error\n",
      "propagation, Chapter 8 (318-362) in Parallel Distributed Processing I D. E. Rumelhart and J. L.\n",
      "McClelland (eds.) MIT Press.\n",
      "\n",
      "Seber, G.  A. F. and Wild, C. J.  (1989). Nonlinear Regression 35-36 Wiley.\n",
      "Sejnowski, T. J., and Rosenberg, C. R.  (1987). Parallel networks that learn to pronounce English text,\n",
      "\n",
      "Complex Syslems  I, 145-168.\n",
      "\n",
      "Thrun, S.  B.  and 23 co-authors (1991).  The MONK's Problems - A perfonnance comparison of different\n",
      "learning algorithms, CMU-CS-91-197 Carnegie-Mellon U.  Department of Computer ScienceTech\n",
      "Report.\n",
      "\n",
      "Towell, G.  and Shavlik, J. W. (1992).  Interpretation of artificial neural networks:  Mapping knowledge\n",
      "\n",
      "based neural networks into rules, in Proceedings of the Neural In/ormation Processing Systems-4, ].\n",
      "E.  Moody, D. S. Touretzky and R. P. Lippmann (eds.) 977-984, Morgan-Kaufmann.\n",
      "\n",
      " model output {\"references\": [\n",
      "    {\n",
      "        \"title\": \"Hassibi, B. Stork, D. G. and Wolff, G.\",\n",
      "        \"authors\": [\"B. Hassibi\", \"D. G. Stork\", \"G. Wolff\"],\n",
      "        \"date\": \"1993a\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Hassibi, B. Stork, D. G. and Wolff, G.\",\n",
      "        \"authors\": [\"B. Hassibi\", \"D. G. Stork\", \"G. Wolff\"],\n",
      "        \"date\": \"1993b\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Hertz, J., Krogh, A. and Palmer, R. G.\",\n",
      "        \"authors\": [\"J. Hertz\", \"A. Krogh\", \"R. G. Palmer\"],\n",
      "        \"date\": \"1991\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Kailath, T.  (1980). Linear Systems Prentice-Hall.\",\n",
      "        \"authors\": [\"T. Kailath\"], \"date\": \"1980\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Kung, S. Y. and Hu, Y. H.  (1991). A Frobenius approximation reduction method (FARM) for determining the optimal number of hidden units, Proceeding of the IJCNN-9I Seattle, Washington.\",\n",
      "        \"authors\": [\"S. Y. Kung\", \"Y. H. Hu\"], \"date\": \"1991\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Le Cun, Y., Denker, J. S. and SoUa, S. A. (1990). Optimal Brain Damage, in Proceedings of the Neural Information Processing Systems-2, D. S. Touretzky (ed.) 598-605, Morgan-Kaufmann.\",\n",
      "        \"authors\": [\"Y. Le Cun\", \"J. S. Denker\", \"S. A. SoUa\"], \"date\": \"1990\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Rissanen, J. (1978). Modelling by shortest data description, Aulomatica 14,465-471.\",\n",
      "        \"authors\": [\"J. Rissanen\"], \"date\": \"1978\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal representations by error propagation, Chapter 8 (318-362) in Parallel Distributed Processing I D. E. Rumelhart and J. L. McClelland (eds.) MIT Press.\",\n",
      "        \"authors\": [\"D. E. Rumelhart\", \"G. E. Hinton\", \"R. J. Williams\"], \"date\": \"1986\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Seber, G.  A. F. and Wild, C. J. (1989). Nonlinear Regression 35-36 Wiley.\",\n",
      "        \"authors\": [\"G. A. F. Seber\", \"C. J. Wild\"], \"date\": \"1989\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Sejnowski, T. J., and Rosenberg, C. R. (1987). Parallel networks that learn to pronounce English text, Complex Systems  I, 145-168.\",\n",
      "        \"authors\": [\"T. J. Sejnowski\", \"C. R. Rosenberg\"], \"date\": \"1987\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Thrun, S. B. and 23 co-authors (1991). The MONK's Problems - A performance comparison of different learning algorithms, CMU-CS-91-197 Carnegie-Mellon U. Department of Computer ScienceTech Report.\",\n",
      "        \"authors\": [\"S. B. Thrun\", \"X.\", \"Y.\", \"23 co-authors\"], \"date\": \"1991\",\n",
      "        \"journal\": null\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Towell, G. and Shavlik, J. W. (1992). Interpretation of artificial neural networks: Mapping knowledge based neural networks into rules, in Proceedings of the Neural Information Processing Systems-4, ].\",\n",
      "        \"authors\": [\"G. Toll\", \"J. W. Shavlik\"], \"date\": \"1992\",\n",
      "        \"journal\": null\n",
      "    }\n",
      "    ]\n",
      "}\n",
      "      \t\t \t\t\t\t\t\t\t\t\t\t\t\n",
      "titles_count 12\n",
      "[1] S. Arora, Y. Liang, and T. Ma. A simple but tough-to-beat baseline for sentence embeddings. 2016.\n",
      "\n",
      "8\n",
      "\n",
      "\f[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n",
      "\n",
      "[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In\n",
      "\n",
      "Advances in neural information processing systems, pages 153–160, 2007.\n",
      "\n",
      "[4] L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo. The ﬁfth pascal recognizing textual entailment\n",
      "\n",
      "challenge. In TAC, 2009.\n",
      "\n",
      "[5] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural\n",
      "\n",
      "language inference. EMNLP, 2015.\n",
      "\n",
      "[6] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. Semeval-2017 task 1: Semantic textual\n",
      "similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.\n",
      "\n",
      "[7] S. Chaturvedi, H. Peng, and D. Roth. Story comprehension for predicting what happens next. In Proceedings\n",
      "of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1603–1614, 2017.\n",
      "\n",
      "[8] D. Chen and C. Manning. A fast and accurate dependency parser using neural networks. In Proceedings\n",
      "of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740–750,\n",
      "2014.\n",
      "\n",
      "[9] Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora question pairs. https://data.quora.com/First-Quora-\n",
      "\n",
      "Dataset-Release-Question-Pairs, 2018.\n",
      "\n",
      "[10] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural networks\n",
      "with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages\n",
      "160–167. ACM, 2008.\n",
      "\n",
      "[11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing\n",
      "\n",
      "(almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493–2537, 2011.\n",
      "\n",
      "[12] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes. Supervised learning of universal sentence\n",
      "\n",
      "representations from natural language inference data. EMNLP, 2017.\n",
      "\n",
      "[13] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in Neural Information Processing\n",
      "\n",
      "Systems, pages 3079–3087, 2015.\n",
      "\n",
      "[14] W. B. Dolan and C. Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings\n",
      "\n",
      "of the Third International Workshop on Paraphrasing (IWP2005), 2005.\n",
      "\n",
      "[15] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised\n",
      "pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625–660, 2010.\n",
      "\n",
      "[16] S. Gray, A. Radford, and K. P. Diederik. Gpu kernels for block-sparse weights. 2017.\n",
      "\n",
      "[17] Z. He, S. Liu, M. Li, M. Zhou, L. Zhang, and H. Wang. Learning entity representation for entity disam-\n",
      "biguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 2: Short Papers), volume 2, pages 30–34, 2013.\n",
      "\n",
      "[18] D. Hendrycks and K. Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear\n",
      "\n",
      "units. arXiv preprint arXiv:1606.08415, 2016.\n",
      "\n",
      "[19] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching\n",
      "machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1693–\n",
      "1701, 2015.\n",
      "\n",
      "[20] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural\n",
      "\n",
      "computation, 18(7):1527–1554, 2006.\n",
      "\n",
      "[21] J. Howard and S. Ruder. Universal language model ﬁne-tuning for text classiﬁcation. Association for\n",
      "\n",
      "Computational Linguistics (ACL), 2018.\n",
      "\n",
      "[22] Y. Jernite, S. R. Bowman, and D. Sontag. Discourse-based objectives for fast unsupervised sentence\n",
      "\n",
      "representation learning. arXiv preprint arXiv:1705.00557, 2017.\n",
      "\n",
      "[23] Y. Ji and J. Eisenstein. Discriminative improvements to distributional sentence similarity. In Proceedings\n",
      "\n",
      "of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, 2013.\n",
      "\n",
      "9\n",
      "\n",
      "\f[24] F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuurmans. Semi-supervised conditional random ﬁelds\n",
      "for improved sequence segmentation and labeling. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,\n",
      "pages 209–216. Association for Computational Linguistics, 2006.\n",
      "\n",
      "[25] T. Khot, A. Sabharwal, and P. Clark. Scitail: A textual entailment dataset from science question answering.\n",
      "\n",
      "In Proceedings of AAAI, 2018.\n",
      "\n",
      "[26] Y. Kim. Convolutional neural networks for sentence classiﬁcation. EMNLP, 2014.\n",
      "\n",
      "[27] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n",
      "\n",
      "2014.\n",
      "\n",
      "[28] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. Skip-thought\n",
      "\n",
      "vectors. In Advances in neural information processing systems, pages 3294–3302, 2015.\n",
      "\n",
      "[29] N. Kitaev and D. Klein. Constituency parsing with a self-attentive encoder. ACL, 2018.\n",
      "\n",
      "[30] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. Race: Large-scale reading comprehension dataset from\n",
      "\n",
      "examinations. EMNLP, 2017.\n",
      "\n",
      "[31] G. Lample, L. Denoyer, and M. Ranzato. Unsupervised machine translation using monolingual corpora\n",
      "\n",
      "only. ICLR, 2018.\n",
      "\n",
      "[32] Q. Le and T. Mikolov. Distributed representations of sentences and documents. In International Conference\n",
      "\n",
      "on Machine Learning, pages 1188–1196, 2014.\n",
      "\n",
      "[33] P. Liang. Semi-supervised learning for natural language. PhD thesis, Massachusetts Institute of Technology,\n",
      "\n",
      "2005.\n",
      "\n",
      "[34] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by\n",
      "\n",
      "summarizing long sequences. ICLR, 2018.\n",
      "\n",
      "[35] X. Liu, K. Duh, and J. Gao. Stochastic answer networks for natural language inference. arXiv preprint\n",
      "\n",
      "arXiv:1804.07888, 2018.\n",
      "\n",
      "[36] L. Logeswaran and H. Lee. An efﬁcient framework for learning sentence representations. ICLR, 2018.\n",
      "\n",
      "[37] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101,\n",
      "\n",
      "2017.\n",
      "\n",
      "[38] B. McCann, J. Bradbury, C. Xiong, and R. Socher. Learned in translation: Contextualized word vectors. In\n",
      "\n",
      "Advances in Neural Information Processing Systems, pages 6297–6308, 2017.\n",
      "\n",
      "[39] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words\n",
      "and phrases and their compositionality. In Advances in neural information processing systems, pages\n",
      "3111–3119, 2013.\n",
      "\n",
      "[40] N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and J. Allen. Lsdsem 2017 shared task: The story cloze\n",
      "test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level\n",
      "Semantics, pages 46–51, 2017.\n",
      "\n",
      "[41] K. Nigam, A. McCallum, and T. Mitchell. Semi-supervised text classiﬁcation using em. Semi-Supervised\n",
      "\n",
      "Learning, pages 33–56, 2006.\n",
      "\n",
      "[42] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings\n",
      "of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543,\n",
      "2014.\n",
      "\n",
      "[43] M. E. Peters, W. Ammar, C. Bhagavatula, and R. Power. Semi-supervised sequence tagging with bidirec-\n",
      "\n",
      "tional language models. ACL, 2017.\n",
      "\n",
      "[44] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextual-\n",
      "\n",
      "ized word representations. NAACL, 2018.\n",
      "\n",
      "[45] Y. Qi, D. S. Sachan, M. Felix, S. J. Padmanabhan, and G. Neubig. When and why are pre-trained word\n",
      "\n",
      "embeddings useful for neural machine translation? NAACL, 2018.\n",
      "\n",
      "10\n",
      "\n",
      "\f[46] A. Rahman and V. Ng. Resolving complex cases of deﬁnite pronouns: the winograd schema challenge. In\n",
      "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and\n",
      "Computational Natural Language Learning, pages 777–789. Association for Computational Linguistics,\n",
      "2012.\n",
      "\n",
      "[47] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension\n",
      "\n",
      "of text. EMNLP, 2016.\n",
      "\n",
      "[48] P. Ramachandran, P. J. Liu, and Q. V. Le. Unsupervised pretraining for sequence to sequence learning.\n",
      "\n",
      "arXiv preprint arXiv:1611.02683, 2016.\n",
      "\n",
      "[49] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an\n",
      "energy-based model. In Advances in neural information processing systems, pages 1137–1144, 2007.\n",
      "\n",
      "[50] M. Rei. Semi-supervised multitask learning for sequence labeling. ACL, 2017.\n",
      "\n",
      "[51] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics,\n",
      "\n",
      "pages 400–407, 1951.\n",
      "\n",
      "[52] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Koˇcisk`y, and P. Blunsom. Reasoning about entailment\n",
      "\n",
      "with neural attention. arXiv preprint arXiv:1509.06664, 2015.\n",
      "\n",
      "[53] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. arXiv\n",
      "\n",
      "preprint arXiv:1508.07909, 2015.\n",
      "\n",
      "[54] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for\n",
      "semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical\n",
      "methods in natural language processing, pages 1631–1642, 2013.\n",
      "\n",
      "[55] S. Srinivasan, R. Arora, and M. Riedl. A simple and effective approach to the story cloze test. arXiv\n",
      "\n",
      "preprint arXiv:1803.05547, 2018.\n",
      "\n",
      "[56] S. Subramanian, A. Trischler, Y. Bengio, and C. J. Pal. Learning general purpose distributed sentence\n",
      "\n",
      "representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.\n",
      "\n",
      "[57] J. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-word scale\n",
      "\n",
      "unlabeled data. Proceedings of ACL-08: HLT, pages 665–673, 2008.\n",
      "\n",
      "[58] Y. Tay, L. A. Tuan, and S. C. Hui. A compare-propagate architecture with alignment factorization for\n",
      "\n",
      "natural language inference. arXiv preprint arXiv:1801.00102, 2017.\n",
      "\n",
      "[59] Y. Tay, L. A. Tuan, and S. C. Hui. Multi-range reasoning for machine comprehension. arXiv preprint\n",
      "\n",
      "arXiv:1803.09074, 2018.\n",
      "\n",
      "[60] J. Tian, Z. Zhou, M. Lan, and Y. Wu. Ecnu at semeval-2017 task 1: Leverage kernel-based traditional nlp\n",
      "features and neural networks to build a universal model for multilingual and cross-lingual semantic textual\n",
      "similarity. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017),\n",
      "pages 191–197, 2017.\n",
      "\n",
      "[61] Y. Tsvetkov. Opportunities and challenges in working with low-resource languages. CMU, 2017.\n",
      "\n",
      "[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\n",
      "Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010, 2017.\n",
      "\n",
      "[63] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with\n",
      "denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages\n",
      "1096–1103. ACM, 2008.\n",
      "\n",
      "[64] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and\n",
      "\n",
      "analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n",
      "\n",
      "[65] A. Warstadt, A. Singh, and S. R. Bowman. Corpus of linguistic acceptability. http://nyu-mll.github.io/cola,\n",
      "\n",
      "2018.\n",
      "\n",
      "[66] A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence understanding\n",
      "\n",
      "through inference. NAACL, 2018.\n",
      "\n",
      "[67] Y. Xu, J. Liu, J. Gao, Y. Shen, and X. Liu. Towards human-level machine reading comprehension:\n",
      "\n",
      "Reasoning and inference with multiple strategies. arXiv preprint arXiv:1711.04964, 2017.\n",
      "\n",
      "11\n",
      "\n",
      "\f[68] D. Yu, L. Deng, and G. Dahl. Roles of pre-training and ﬁne-tuning in context-dependent dbn-hmms for\n",
      "real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature\n",
      "Learning, 2010.\n",
      "\n",
      "[69] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel\n",
      "\n",
      "prediction. In CVPR, volume 1, page 6, 2017.\n",
      "\n",
      "[70] X. Zhu. Semi-supervised learning literature survey. 2005.\n",
      "\n",
      "[71] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and\n",
      "movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of\n",
      "the IEEE international conference on computer vision, pages 19–27, 2015.\n",
      "\n",
      "12\n",
      "\n",
      " model output {\"references\": [\n",
      "         {\"title\": \"Y. Tay, L. A. Tuan, and S. C. Hui. A compare-propagate architecture with alignment factorization for natural language inference.\",\n",
      "         \"authors\": [\"Y. Tay\", \"L. A. Tuan\", \"S. C. Hui\"],\n",
      "         \"date\": \"2017-12-13\",\n",
      "         \"journal\": null},\n",
      "         {\"title\": \"A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.\",\n",
      "         \"authors\": [\"A. Wang\", \"A. Singh\", \"J. Michael\", \"F. Hill\", \"O. Levy\", \"S. R. Bowman\"],\n",
      "         \"date\": \"2018-03-25\",\n",
      "         \"journal\": null},\n",
      "         {\"title\": \"Y. Wang, L. Deng, and G. Dahl. Roles of pre-training and ﬁne-tuning in context-dependent dbn-hmms for real-world speech recognition.\",\n",
      "         \"authors\": [\"Y. Wang\", \"L. Deng\", \"G. Dahl\"],\n",
      "         \"date\": \"2010-09-30\",\n",
      "         \"journal\": null}\n",
      "    ]}\n",
      "     \t\t\t\t\t\t\t\t\t\t\t\t\t\t \n",
      "titles_count 3\n",
      "[1] Abien Fred Agarap. Deep learning using rectiﬁed linear units\n",
      "\n",
      "(relu). arXiv preprint arXiv:1803.08375, 2018.\n",
      "\n",
      "[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,\n",
      "Alessandro Cappelli, Ruxandra Cojocaru, Merouane Deb-\n",
      "bah, Etienne Gofﬁnet, Daniel Heslow, Julien Launay, Quentin\n",
      "Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme\n",
      "Penedo. Falcon-40B: an open large language model with state-\n",
      "of-the-art performance. 2023.\n",
      "\n",
      "[3] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ah-\n",
      "mad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase,\n",
      "Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-\n",
      "inference: enabling efﬁcient inference of transformer models\n",
      "at unprecedented scale. In SC22: International Conference\n",
      "for High Performance Computing, Networking, Storage and\n",
      "Analysis, pages 1–15. IEEE, 2022.\n",
      "\n",
      "[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.\n",
      "Piqa: Reasoning about physical commonsense in natural lan-\n",
      "\n",
      "guage. In Proceedings of the AAAI conference on artiﬁcial\n",
      "intelligence, volume 34, pages 7432–7439, 2020.\n",
      "\n",
      "[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\n",
      "biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\n",
      "Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\n",
      "models are few-shot learners. Advances in neural information\n",
      "processing systems, 33:1877–1901, 2020.\n",
      "\n",
      "[6] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and\n",
      "Tri Dao. Medusa: Simple framework for accelerating llm gen-\n",
      "eration with multiple decoding heads. https://github.com/\n",
      "FasterDecoding/Medusa, 2023.\n",
      "\n",
      "[7] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-\n",
      "Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerat-\n",
      "ing large language model decoding with speculative sampling,\n",
      "2023.\n",
      "\n",
      "[8] Weihao Cui, Zhenhua Han, Lingji Ouyang, Yichuan Wang,\n",
      "Ningxin Zheng, Lingxiao Ma, Yuqing Yang, Fan Yang, Jilong\n",
      "Xue, Lili Qiu, Lidong Zhou, Quan Chen, Haisheng Tan, and\n",
      "Minyi Guo. Optimizing dynamic neural networks with brain-\n",
      "storm. In 17th USENIX Symposium on Operating Systems De-\n",
      "sign and Implementation (OSDI 23), pages 797–815, Boston,\n",
      "MA, July 2023. USENIX Association.\n",
      "\n",
      "[9] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbo-\n",
      "transformers: an efﬁcient gpu serving system for transformer\n",
      "models. In Proceedings of the 26th ACM SIGPLAN Symposium\n",
      "on Principles and Practice of Parallel Programming, pages\n",
      "389–402, 2021.\n",
      "\n",
      "[10] Wikimedia Foundation. Wikimedia downloads.\n",
      "\n",
      "[11] Elias Frantar and Dan Alistarh. SparseGPT: Massive language\n",
      "models can be accurately pruned in one-shot. arXiv preprint\n",
      "arXiv:2301.00774, 2023.\n",
      "\n",
      "[12] Elias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, and Dan Alis-\n",
      "tarh. GPTQ: Accurate post-training compression for genera-\n",
      "tive pretrained transformers. arXiv preprint arXiv:2210.17323,\n",
      "2022.\n",
      "\n",
      "[13] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking\n",
      "the sequential dependency of llm inference using lookahead\n",
      "decoding, November 2023.\n",
      "\n",
      "[14] Georgi Gerganov. ggerganov/llama.cpp: Port of facebook’s\n",
      "llama model in c/c++. https://github.com/ggerganov/\n",
      "llama.cpp, 2023.\n",
      "\n",
      "[15] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\n",
      "William B Dolan. The third pascal recognizing textual entail-\n",
      "ment challenge. In Proceedings of the ACL-PASCAL workshop\n",
      "on textual entailment and paraphrasing, pages 1–9, 2007.\n",
      "\n",
      "[16] Song Han, Huizi Mao, and William J Dally. Deep compression:\n",
      "Compressing deep neural networks with pruning, trained quan-\n",
      "tization and huffman coding. arXiv preprint arXiv:1510.00149,\n",
      "2015.\n",
      "\n",
      "[17] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning\n",
      "both weights and connections for efﬁcient neural networks. In\n",
      "Proceedings of the 28th International Conference on Neural\n",
      "Information Processing Systems - Volume 1, NIPS’15, page\n",
      "1135–1143, Cambridge, MA, USA, 2015. MIT Press.\n",
      "\n",
      "13\n",
      "\n",
      "\f[18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lian-\n",
      "min Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and\n",
      "Ion Stoica. Efﬁcient memory management for large language\n",
      "In Proceedings of the\n",
      "model serving with pagedattention.\n",
      "29th Symposium on Operating Systems Principles, SOSP ’23,\n",
      "page 611–626, New York, NY, USA, 2023. Association for\n",
      "Computing Machinery.\n",
      "\n",
      "[19] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li,\n",
      "Ankit Singh Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix\n",
      "Yu, Ruiqi Guo, et al. The lazy neuron phenomenon: On emer-\n",
      "gence of activation sparsity in transformers. In The Eleventh\n",
      "International Conference on Learning Representations, 2022.\n",
      "\n",
      "[20] Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin\n",
      "Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. Do emergent\n",
      "abilities exist in quantized large language models: An empirical\n",
      "study, 2023.\n",
      "\n",
      "[21] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,\n",
      "Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian,\n",
      "Christopher Re, and Beidi Chen. Deja vu: Contextual spar-\n",
      "sity for efﬁcient LLMs at inference time. In Andreas Krause,\n",
      "Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan\n",
      "Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th\n",
      "International Conference on Machine Learning, volume 202\n",
      "of Proceedings of Machine Learning Research, pages 22137–\n",
      "22176. PMLR, 23–29 Jul 2023.\n",
      "\n",
      "[22] Hanjia Lyu, Song Jiang, Hanqing Zeng, Qifan Wang, Si Zhang,\n",
      "Ren Chen, Chris Leung, Jiajie Tang, Yinglong Xia, and Jiebo\n",
      "Luo. Llm-rec: Personalized recommendation via prompting\n",
      "large language models, 2023.\n",
      "\n",
      "[23] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner:\n",
      "On the structural pruning of large language models. arXiv\n",
      "preprint arXiv:2305.11627, 2023.\n",
      "\n",
      "[24] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner:\n",
      "On the structural pruning of large language models. In Ad-\n",
      "vances in Neural Information Processing Systems, 2023.\n",
      "\n",
      "[25] Iván Martínez Toro, Daniel Gallego Vico, and Pablo Orgaz.\n",
      "\n",
      "PrivateGPT, May 2023.\n",
      "\n",
      "[26] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,\n",
      "Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaox-\n",
      "iang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna\n",
      "Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative\n",
      "large language model serving with speculative inference and\n",
      "token tree veriﬁcation, 2023.\n",
      "\n",
      "[27] Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del\n",
      "Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari,\n",
      "and Mehrdad Farajtabar. Relu strikes back: Exploiting activa-\n",
      "tion sparsity in large language models, 2023.\n",
      "\n",
      "[28] MohamedRashad.\n",
      "\n",
      "https://huggingface.co/datasets/\n",
      "\n",
      "MohamedRashad/ChatGPT-prompts, 2023.\n",
      "\n",
      "[29] NVIDIA.\n",
      "\n",
      "Uniﬁed memory programming.\n",
      "\n",
      "https:\n",
      "\n",
      "//docs.nvidia.com/cuda/cuda-c-programming-guide/\n",
      "index.html#um-unified-memory-programming-hd, 2021.\n",
      "\n",
      "[30] NVIDIA. cuSPARSE: Basic Linear Algebra for Sparse Ma-\n",
      "trices on NVIDIA GPUs. https://developer.nvidia.com/\n",
      "cusparse, 2023.\n",
      "\n",
      "[31] OpenAI. https://openai.com/blog/chatgpt, 2023.\n",
      "\n",
      "[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\n",
      "Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\n",
      "Peter J Liu. Exploring the limits of transfer learning with\n",
      "a uniﬁed text-to-text transformer. The Journal of Machine\n",
      "Learning Research, 21(1):5485–5551, 2020.\n",
      "\n",
      "[33] Google Research. Sputnik: a library of sparse linear algebra\n",
      "kernels and utilities for deep learning. https://github.com/\n",
      "google-research/sputnik, 2023.\n",
      "\n",
      "[34] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gor-\n",
      "don. Choice of plausible alternatives: An evaluation of com-\n",
      "monsense causal reasoning. In 2011 AAAI Spring Symposium\n",
      "Series, 2011.\n",
      "\n",
      "[35] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and\n",
      "Yejin Choi. Winogrande: An adversarial winograd schema\n",
      "challenge at scale. Communications of the ACM, 64(9):99–\n",
      "106, 2021.\n",
      "\n",
      "[36] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper,\n",
      "Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu,\n",
      "Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, and Ion\n",
      "Stoica. S-lora: Serving thousands of concurrent lora adapters,\n",
      "2023.\n",
      "\n",
      "[37] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max\n",
      "Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica,\n",
      "and Ce Zhang. Flexgen: High-throughput generative inference\n",
      "of large language models with a single gpu. 2023.\n",
      "\n",
      "[38] SparseLLM. Relufalcon-40b. https://huggingface.co/\n",
      "\n",
      "SparseLLM/ReluFalcon-40B.\n",
      "\n",
      "[39] SparseLLM. Relullama-70b. https://huggingface.co/\n",
      "\n",
      "SparseLLM/ReluLLaMA-70B.\n",
      "\n",
      "[40] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A sim-\n",
      "ple and effective pruning approach for large language models.\n",
      "arXiv preprint arXiv:2306.11695, 2023.\n",
      "\n",
      "[41] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n",
      "Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\n",
      "Stanford alpaca:\n",
      "and Tatsunori B. Hashimoto.\n",
      "https:\n",
      "An\n",
      "instruction-following\n",
      "//github.com/tatsu-lab/stanford_alpaca, 2023.\n",
      "\n",
      "llama model.\n",
      "\n",
      "[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\n",
      "tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roz-\n",
      "ière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:\n",
      "Open and efﬁcient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971, 2023.\n",
      "\n",
      "[43] Yiding Wang, Kai Chen, Haisheng Tan, and Kun Guo. Tabi:\n",
      "An efﬁcient multi-level inference system for large language\n",
      "models. In Proceedings of the Eighteenth European Confer-\n",
      "ence on Computer Systems, EuroSys ’23, page 233–248, New\n",
      "York, NY, USA, 2023. Association for Computing Machinery.\n",
      "\n",
      "[44] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\n",
      "Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault,\n",
      "Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\n",
      "Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Can-\n",
      "wen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\n",
      "Quentin Lhoest, and Alexander M. Rush. Transformers: State-\n",
      "of-the-art natural language processing. In Proceedings of the\n",
      "\n",
      "14\n",
      "\n",
      "\f2020 Conference on Empirical Methods in Natural Language\n",
      "Processing: System Demonstrations, pages 38–45, Online, Oc-\n",
      "tober 2020. Association for Computational Linguistics.\n",
      "[45] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang,\n",
      "Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuai-\n",
      "wen Leon Song. Flash-llm: Enabling cost-effective and highly-\n",
      "efﬁcient large generative model inference with unstructured\n",
      "sparsity, 2023.\n",
      "\n",
      "[46] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien De-\n",
      "mouth, and Song Han. Smoothquant: Accurate and efﬁcient\n",
      "post-training quantization for large language models. In In-\n",
      "ternational Conference on Machine Learning, pages 38087–\n",
      "38099. PMLR, 2023.\n",
      "\n",
      "[47] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong\n",
      "Kim, and Byung-Gon Chun. Orca: A distributed serving\n",
      "In 16th\n",
      "system for Transformer-Based generative models.\n",
      "USENIX Symposium on Operating Systems Design and Im-\n",
      "plementation (OSDI 22), pages 521–538, Carlsbad, CA, July\n",
      "2022. USENIX Association.\n",
      "\n",
      "[48] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham\n",
      "Shah, Charles Robert Jankowski, Deqing Yang, and Yanghua\n",
      "Xiao. Distilling script knowledge from large language\n",
      "models for constrained language planning. arXiv preprint\n",
      "arXiv:2305.05252, 2023.\n",
      "\n",
      "[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\n",
      "Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\n",
      "\n",
      "Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\n",
      "former language models. arXiv preprint arXiv:2205.01068,\n",
      "2022.\n",
      "\n",
      "[50] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong\n",
      "Sun, and Jie Zhou. MoEﬁcation: Transformer feed-forward\n",
      "layers are mixtures of experts. In Findings of ACL 2022, 2022.\n",
      "\n",
      "[51] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han,\n",
      "Lingxiao Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang,\n",
      "Lili Qiu, Mao Yang, and Lidong Zhou. Pit: Optimization of\n",
      "dynamic sparse deep learning models via permutation invariant\n",
      "In Proceedings of the 29th Symposium on\n",
      "transformation.\n",
      "Operating Systems Principles, SOSP ’23, page 331–347, New\n",
      "York, NY, USA, 2023. Association for Computing Machinery.\n",
      "\n",
      "[52] Ningxin Zheng, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing\n",
      "Yang, Fan Yang, Yang Wang, Mao Yang, and Lidong Zhou.\n",
      "SparTA: Deep-Learning model sparsity via Tensor-with-\n",
      "Sparsity-Attribute. In 16th USENIX Symposium on Operating\n",
      "Systems Design and Implementation (OSDI 22), pages 213–\n",
      "232, Carlsbad, CA, July 2022. USENIX Association.\n",
      "\n",
      "[53] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun.\n",
      "PetS: A uniﬁed framework for Parameter-Efﬁcient transform-\n",
      "ers serving. In 2022 USENIX Annual Technical Conference\n",
      "(USENIX ATC 22), pages 489–504, Carlsbad, CA, July 2022.\n",
      "USENIX Association.\n",
      "\n",
      "15\n",
      "\n",
      " model output {\"references\": [\n",
      "        { \"title\": \"Zhang et al. (2015)\", \"authors\": [\"Xiaoying Zhang\", \"Yanming ...\", \"Ji...\"], \"date\": \"2015-01-01\",\n",
      "         \"journal\": \"None\"\n",
      "    }]}\n",
      "     \t \t  \t \t  \t \t  \n",
      "titles_count 1\n",
      "[1] Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022. Towards a cleaner document-oriented multilingual crawled corpus.\n",
      "\n",
      "arXiv preprint arXiv:2201.06642 (2022).\n",
      "\n",
      "[2] Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2007. Large language models in machine translation. (2007).\n",
      "[3] Ethan C Chau, Lucy H Lin, and Noah A Smith. 2020. Parsing with multilingual BERT, a small corpus, and a small treebank. arXiv preprint\n",
      "\n",
      "arXiv:2009.14124 (2020).\n",
      "\n",
      "[4] Monojit Choudhury. 2023. Generative AI has a language problem. Nature Human Behaviour (2023), 1–2.\n",
      "[5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint\n",
      "\n",
      "arXiv:2305.14314 (2023).\n",
      "\n",
      "[6] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing Chat\n",
      "\n",
      "Language Models by Scaling High-quality Instructional Conversations. arXiv preprint arXiv:2305.14233 (2023).\n",
      "\n",
      "[7] Free Dolly. 2023. Introducing the World’s First Truly Open Instruction-Tuned LLM. databricks. com.\n",
      "[8] Yunhui Guo. 2018. A survey on methods and theories of quantized neural networks. arXiv preprint arXiv:1808.04752 (2018).\n",
      "[9] Claudiu D. Hromei, Danilo Croce, Valerio Basile, and Roberto Basili. 2023. ExtremITA at EVALITA 2023: Multi-Task Sustainable Scaling to Large\n",
      "Language Models at its Extreme. In Eighth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA\n",
      "2023), Vol. 3473. CEUR.\n",
      "\n",
      "[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation\n",
      "\n",
      "of large language models. arXiv preprint arXiv:2106.09685 (2021).\n",
      "\n",
      "[11] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023. LLM-Adapters: An Adapter\n",
      "\n",
      "Family for Parameter-Efficient Fine-Tuning of Large Language Models. arXiv preprint arXiv:2304.01933 (2023).\n",
      "\n",
      "[12] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language\n",
      "\n",
      "understanding. arXiv preprint arXiv:1909.10351 (2019).\n",
      "\n",
      "[13] Mirko Lai, Stefano Menini, Marco Polignano, Valentina Russo, Rachele Sprugnoli, and Giulia Venturi. 2023. Evalita 2023: Overview of the 8th\n",
      "evaluation campaign of natural language processing and speech tools for italian. In Proceedings of the Eighth Evaluation Campaign of Natural\n",
      "Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2023), CEUR. org, Parma, Italy.\n",
      "\n",
      "[14] Mirko Lai, Stefano Menini, Marco Polignano, Valentina Russo, Rachele Sprugnoli, and Giulia Venturi. 2023. EVALITA 2023: Overview of the 8th\n",
      "Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. In Eighth Evaluation Campaign of Natural Language Processing\n",
      "and Speech Tools for Italian. Final Workshop (EVALITA 2023), Vol. 3473. CEUR.\n",
      "\n",
      "[15] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. 2023. LLM-FP4: 4-Bit Floating-Point Quantized Transformers.\n",
      "\n",
      "arXiv preprint arXiv:2310.16836 (2023).\n",
      "\n",
      "[16] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. 2023.\n",
      "\n",
      "Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology (2023), 100017.\n",
      "\n",
      "[17] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas\n",
      "\n",
      "Chandra. 2023. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models. arXiv:2305.17888 [cs.CL]\n",
      "\n",
      "[18] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n",
      "[19] Marco Polignano, Pierpaolo Basile, Marco De Gemmis, Giovanni Semeraro, Valerio Basile, et al. 2019. Alberto: Italian BERT language understanding\n",
      "\n",
      "model for NLP challenging tasks based on tweets. In CEUR Workshop Proceedings, Vol. 2481. CEUR, 1–6.\n",
      "\n",
      "[20] Gabriele Sarti and Malvina Nissim. 2022. It5: Large-scale text-to-text pretraining for italian language understanding and generation. arXiv preprint\n",
      "\n",
      "arXiv:2203.03759 (2022).\n",
      "\n",
      "[21] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language\n",
      "\n",
      "models. arXiv preprint arXiv:2102.02503 (2021).\n",
      "\n",
      "[22] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford\n",
      "\n",
      "Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.\n",
      "\n",
      "[23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\n",
      "\n",
      "Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).\n",
      "\n",
      "[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is\n",
      "\n",
      "all you need. Advances in neural information processing systems 30 (2017).\n",
      "\n",
      "[25] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et al. 2020. K-adapter: Infusing\n",
      "\n",
      "knowledge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808 (2020).\n",
      "\n",
      "[26] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,\n",
      "\n",
      "et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022).\n",
      "\n",
      "\f14\n",
      "\n",
      "Basile, et al.\n",
      "\n",
      "[27] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan\n",
      "\n",
      "Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n",
      "\n",
      "[28] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha\n",
      "\n",
      "Luccioni, François Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n",
      "\n",
      "[29] Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang\n",
      "Sutawika, Jungo Kasai, Ahmed Baruwa, et al. 2022. Bloom+ 1: Adding language support to bloom for zero-shot prompting. arXiv preprint\n",
      "arXiv:2212.09535 (2022).\n",
      "\n",
      "[30] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.\n",
      "\n",
      "A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\n",
      "\n",
      " model output {\"references\": [\n",
      "        {\"title\": \"BigScience Workshop, Teven Le Scao\", \"authors\": [\"Teven Le Saco\", \"Angela Fan\"], \"date\": \"2022-12-25\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Jason Wang, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, ...\", \"authors\": [\n",
      "            \"Jason Wang\", \"Yi Tay\", \"Rishi Bommasani\", \"Colin Raffel\",\n",
      "            \"Barret Zoph\", \"Sebastian Borgeaud\", \"Xuechen Li\", \"Carlos Guestrin\",\n",
      "            \"Percy L. Liang\", \"Tatsunori B. Hashimoto\"], \"date\": \"2023-10-24\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Huggingface’s transformers: State-of-the-art natural language processing\", \"authors\": [\"Thomas Wolf\", \"Lysandre Debut\", \"Victor Sanh\", \" Julien Chaumond\", \"Anthony Moi\", \"Pierric Cistac\", \"Tim Rault\", \"Rémi Louf\", \"Morgan Funtowicz\", \"_et al.\"], \"date\": \"2019-10-13\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"BigScience Workshop, Teven Le Saco\", \"authors\": [\"Teven Le Saco\", \"Angela Fan\"], \"date\": \"2022-12-25\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Jason Wang, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, ...\", \"authors\": [\n",
      "            \"Jason Wang\", \"Yi Tay\", \"Rishi Bommasani\", \"Colin Raffel\",\n",
      "            \"Barret Zoph\", \"Sebastian Borgeaud\", \"Xuechen Li\", \"Carlos Guestrin\", \"Percy L. Liang\", \"Tatsunori B. Hashimoto\"], \"date\": \"2023-10-24\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Huggingface’s transformers: State-of-the-art natural language processing\", \"authors\": [\"Thomas Wolf\", \"Lysandre Debut\", \"Victor Sanh\", \" Julien Chaumond\", \"Anthony Moi\", \"Pierric Cistac\", \"Tim Rault\", \"Rémi Louf\", \"Morgan Funtowicz\", \"_et al.\", \"et al.\"], \"date\": \"2019-10-13\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"BigScience Workshop, Teven Le Saco\", \"authors\": [\"Teven Le Saco\", \"Angela Fan\"], \"date\": \"2022-12-25\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Jason Wang, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, ...\", \"authors\": [\n",
      "            \"Jason Wang\", \"Yi Tay\", \"Rishi Bommasani\", \"Colin Raffel\",\n",
      "            \"Barret Zoph\", \"Sebastian Borgeaud\", \"Xuechen Li\", \"Carlos Guestrin\", \"Percy L. Liang\", \"Tatsunori B. Hashimoto\"], \"date\": \"2023-10-24\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Huggingface’s transformers: State-of-the-art natural language processing\", \"authors\": [\"Thomas Wolf\", \"Lysandre Debut\", \"Victor Sanh\", \" Julien Chaumond\", \"Anthony Moi\", \"Pierric Cistac\", \"Tim Rault\", \"Rémi Louf\", \"Morgan Funtowicz\", \"_et al.\", \"et al.\"], \"date\": \"2019-10-13\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"BigScience Workshop, Teven Le Saco\", \"authors\": [\"Teven Le Saco\", \"Angela Fan\"], \"date\": \"2022-12-25\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Jason Wang, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, ...\", \"authors\": [\n",
      "            \"Jason Wang\", \"Yi Tay\", \"Rishi Bommasani\", \"Colin Raffel\",\n",
      "            \"Barret Zoph\", \"Sebastian Borgeaud\", \"Xuechen Li\", \"Carlos Guestrin\", \"Percy L. Liang\", \"Tatsunori B. Hashimoto\"], \"date\": \"2023-10-24\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"Huggingface’s transformers: State-of-the-art natural language processing\", \"authors\": [\"Thomas Wolf\", \"Lysandre Debut\", \"Victor Sanh\", \" Julien Chaumond\", \"Anthony Moi\", \"Pierric Cistac\", \"Tim Rault\", \"Rémi Louf\", \"Morgan Funtowicz\", \"_et al.\", \"et al.\"], \"date\": \"2019-10-13\",\n",
      "         \"journal\": null},\n",
      "        {\"title\": \"BigScience Workshop, Teven Le Saco\", \"authors\": [\"Teven Le Saco\", \"Angela Fan\"], \"date\": \"2022-12-25\",\n",
      "         \"journal\": null}\n",
      "    ]}\n",
      "   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "titles_count 13\n",
      "[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Am-\n",
      "mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden\n",
      "Smith, Olatunji Ruwase, et al. 2022. DeepSpeed Inference: Enabling\n",
      "Efficient Inference of Transformer Models at Unprecedented Scale.\n",
      "arXiv preprint arXiv:2207.00032 (2022).\n",
      "\n",
      "[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer\n",
      "\n",
      "normalization. arXiv preprint arXiv:1607.06450 (2016).\n",
      "\n",
      "[3] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000. A neural\n",
      "probabilistic language model. Advances in neural information process-\n",
      "ing systems 13 (2000).\n",
      "\n",
      "[4] Ond rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Gra-\n",
      "ham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp\n",
      "Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurelie\n",
      "Neveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Car-\n",
      "olina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos\n",
      "Zampieri. 2016. Findings of the 2016 Conference on Machine Trans-\n",
      "lation. In Proceedings of the First Conference on Machine Translation.\n",
      "Association for Computational Linguistics, Berlin, Germany, 131–198.\n",
      "http://www.aclweb.org/anthology/W/W16/W16-2301\n",
      "\n",
      "[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\n",
      "Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\n",
      "Sastry, Amanda Askell, et al. 2020. Language models are few-shot\n",
      "learners. Advances in neural information processing systems 33 (2020),\n",
      "1877–1901.\n",
      "\n",
      "[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\n",
      "de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas\n",
      "\n",
      "14\n",
      "\n",
      "Joseph, Greg Brockman, et al. 2021. Evaluating large language models\n",
      "trained on code. arXiv preprint arXiv:2107.03374 (2021).\n",
      "\n",
      "[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\n",
      "arXiv preprint\n",
      "\n",
      "Training deep nets with sublinear memory cost.\n",
      "arXiv:1604.06174 (2016).\n",
      "\n",
      "[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao\n",
      "Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E.\n",
      "Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source\n",
      "Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.\n",
      "org/blog/2023-03-30-vicuna/\n",
      "\n",
      "[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\n",
      "Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\n",
      "Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling lan-\n",
      "guage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n",
      "[10] Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion\n",
      "Stoica, Joseph Gonzalez, and Alexey Tumanov. 2020. InferLine: latency-\n",
      "aware provisioning and scaling for prediction serving pipelines. In\n",
      "Proceedings of the 11th ACM Symposium on Cloud Computing. 477–491.\n",
      "[11] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin,\n",
      "Joseph E Gonzalez, and Ion Stoica. 2017. Clipper: A Low-Latency\n",
      "Online Prediction Serving System. In 14th USENIX Symposium on\n",
      "Networked Systems Design and Implementation (NSDI 17). 613–627.\n",
      "[12] Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui Li, Deze Zeng,\n",
      "Chao Li, and Minyi Guo. 2022. DVABatch: Diversity-aware Multi-\n",
      "Entry Multi-Exit Batching for Efficient Processing of DNN Services\n",
      "on GPUs. In 2022 USENIX Annual Technical Conference (USENIX ATC\n",
      "22). 183–198.\n",
      "\n",
      "[13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.\n",
      "2022. Flashattention: Fast and memory-efficient exact attention with\n",
      "io-awareness. Advances in Neural Information Processing Systems 35\n",
      "(2022), 16344–16359.\n",
      "\n",
      "[14] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. 2021. TurboTrans-\n",
      "formers: an efficient GPU serving system for transformer models. In\n",
      "Proceedings of the 26th ACM SIGPLAN Symposium on Principles and\n",
      "Practice of Parallel Programming. 389–402.\n",
      "\n",
      "[15] FastAPI. 2023. FastAPI. https://github.com/tiangolo/fastapi.\n",
      "[16] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency\n",
      "rnn inference with cellular batching. In Proceedings of the Thirteenth\n",
      "EuroSys Conference. 1–15.\n",
      "\n",
      "[17] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and\n",
      "Kurt Keutzer. 2021. Ai and memory wall. RiseLab Medium Post 1 (2021),\n",
      "6.\n",
      "\n",
      "[18] Github. 2022. https://github.com/features/copilot\n",
      "[19] Google. 2023. https://bard.google.com/\n",
      "[20] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf-\n",
      "mann, Ymir Vigfusson, and Jonathan Mace. 2020. Serving {DNNs} like\n",
      "Clockwork: Performance Predictability from the Bottom Up. In 14th\n",
      "USENIX Symposium on Operating Systems Design and Implementation\n",
      "(OSDI 20). 443–462.\n",
      "\n",
      "[21] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo Chen.\n",
      "Microsecond-scale Preemption for Concurrent {GPU-\n",
      "2022.\n",
      "accelerated} {DNN} Inferences. In 16th USENIX Symposium on Oper-\n",
      "ating Systems Design and Implementation (OSDI 22). 539–558.\n",
      "\n",
      "[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep\n",
      "residual learning for image recognition. In Proceedings of the IEEE\n",
      "conference on computer vision and pattern recognition. 770–778.\n",
      "[23] Chien-Chin Huang, Gu Jin, and Jinyang Li. 2020. Swapadvisor: Push-\n",
      "ing deep learning beyond the gpu memory limit via smart swapping.\n",
      "In Proceedings of the Twenty-Fifth International Conference on Archi-\n",
      "tectural Support for Programming Languages and Operating Systems.\n",
      "1341–1355.\n",
      "\n",
      "[24] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter\n",
      "Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. 2020. Check-\n",
      "mate: Breaking the memory wall with optimal tensor rematerialization.\n",
      "\n",
      "\fProceedings of Machine Learning and Systems 2 (2020), 497–511.\n",
      "[25] Tom Kilburn, David BG Edwards, Michael J Lanigan, and Frank H\n",
      "Sumner. 1962. One-level storage system. IRE Transactions on Electronic\n",
      "Computers 2 (1962), 223–235.\n",
      "\n",
      "[26] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power\n",
      "arXiv preprint\n",
      "\n",
      "of scale for parameter-efficient prompt tuning.\n",
      "arXiv:2104.08691 (2021).\n",
      "\n",
      "[27] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing contin-\n",
      "uous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).\n",
      "[28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng,\n",
      "Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez,\n",
      "et al. 2023. AlpaServe: Statistical Multiplexing with Model Parallelism\n",
      "for Deep Learning Serving. arXiv preprint arXiv:2302.11665 (2023).\n",
      "\n",
      "[29] Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei\n",
      "Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. 2020.\n",
      "Rammer: Enabling holistic deep learning compiler optimizations with\n",
      "rtasks. In Proceedings of the 14th USENIX Conference on Operating\n",
      "Systems Design and Implementation. 881–897.\n",
      "\n",
      "[30] NVIDIA. [n. d.]. Triton Inference Server. https://developer.nvidia.com/\n",
      "\n",
      "nvidia-triton-inference-server.\n",
      "\n",
      "[31] NVIDIA. 2023.\n",
      "\n",
      "FasterTransformer. https://github.com/NVIDIA/\n",
      "\n",
      "FasterTransformer.\n",
      "\n",
      "[32] NVIDIA. 2023. NCCL: The NVIDIA Collective Communication Library.\n",
      "\n",
      "https://developer.nvidia.com/nccl.\n",
      "\n",
      "[33] Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li\n",
      "Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke.\n",
      "2017. Tensorflow-serving: Flexible, high-performance ml serving.\n",
      "arXiv preprint arXiv:1712.06139 (2017).\n",
      "\n",
      "[34] OpenAI. 2020. https://openai.com/blog/openai-api\n",
      "[35] OpenAI. 2022. https://openai.com/blog/chatgpt\n",
      "[36] OpenAI. 2023.\n",
      "chatgpt\n",
      "\n",
      "https://openai.com/blog/custom-instructions-for-\n",
      "\n",
      "[37] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n",
      "[38] LMSYS ORG. 2023. Chatbot Arena Leaderboard Week 8: Introduc-\n",
      "ing MT-Bench and Vicuna-33B. https://lmsys.org/blog/2023-06-22-\n",
      "leaderboard/.\n",
      "\n",
      "[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\n",
      "Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia\n",
      "Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style,\n",
      "high-performance deep learning library. Advances in neural informa-\n",
      "tion processing systems 32 (2019).\n",
      "\n",
      "[40] Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica, and Joseph Gon-\n",
      "zalez. 2022. POET: Training Neural Networks on Tiny Devices with\n",
      "Integrated Rematerialization and Paging. In International Conference\n",
      "on Machine Learning. PMLR, 17573–17583.\n",
      "\n",
      "[41] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,\n",
      "James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani\n",
      "Agrawal, and Jeff Dean. 2022. Efficiently Scaling Transformer Inference.\n",
      "arXiv preprint arXiv:2211.05102 (2022).\n",
      "\n",
      "[42] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji\n",
      "Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He.\n",
      "2021. ZeRO-Offload: Democratizing Billion-Scale Model Training.. In\n",
      "USENIX Annual Technical Conference. 551–564.\n",
      "\n",
      "[43] Reuters. 2023. https://www.reuters.com/technology/tech-giants-ai-\n",
      "like-bing-bard-poses-billion-dollar-search-problem-2023-02-22/\n",
      "[44] Amazon Web Services. 2023. https://aws.amazon.com/bedrock/\n",
      "[45] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong,\n",
      "Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019.\n",
      "Nexus: A GPU cluster engine for accelerating DNN-based video anal-\n",
      "ysis. In Proceedings of the 27th ACM Symposium on Operating Systems\n",
      "Principles. 322–337.\n",
      "\n",
      "[46] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,\n",
      "Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gon-\n",
      "zalez, et al. 2023. High-throughput Generative Inference of Large\n",
      "\n",
      "15\n",
      "\n",
      "Language Models with a Single GPU. arXiv preprint arXiv:2303.06865\n",
      "(2023).\n",
      "\n",
      "[47] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\n",
      "Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-\n",
      "billion parameter language models using model parallelism. arXiv\n",
      "preprint arXiv:1909.08053 (2019).\n",
      "\n",
      "[48] Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, and James Hegarty. 2022.\n",
      "OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the\n",
      "Memory Usage of Neural Networks. (2022). https://doi.org/10.48550/\n",
      "arXiv.2210.12924\n",
      "\n",
      "[49] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to se-\n",
      "quence learning with neural networks. Advances in neural information\n",
      "processing systems 27 (2014).\n",
      "\n",
      "[50] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen\n",
      "Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.\n",
      "Stanford Alpaca: An Instruction-following LLaMA model. https://\n",
      "github.com/tatsu-lab/stanford_alpaca.\n",
      "[51] ShareGPT Team. 2023. https://sharegpt.com/\n",
      "[52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\n",
      "Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\n",
      "Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation\n",
      "language models. arXiv preprint arXiv:2302.13971 (2023).\n",
      "\n",
      "[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n",
      "Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. At-\n",
      "tention is all you need. Advances in neural information processing\n",
      "systems 30 (2017).\n",
      "\n",
      "[54] Jing Wang, Youyou Lu, Qing Wang, Minhui Xie, Keji Huang, and Jiwu\n",
      "Shu. 2022. Pacman: An Efficient Compaction Approach for {Log-\n",
      "Structured} {Key-Value} Store on Persistent Memory. In 2022 USENIX\n",
      "Annual Technical Conference (USENIX ATC 22). 773–788.\n",
      "\n",
      "[55] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuai-\n",
      "wen Leon Song, Zenglin Xu, and Tim Kraska. 2018. Superneurons: Dy-\n",
      "namic GPU memory management for training deep neural networks.\n",
      "In Proceedings of the 23rd ACM SIGPLAN symposium on principles and\n",
      "practice of parallel programming. 41–53.\n",
      "\n",
      "[56] Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and Lei Li.\n",
      "2021. LightSeq: A High Performance Inference Library for Transform-\n",
      "ers. In Proceedings of the 2021 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Tech-\n",
      "nologies: Industry Papers. 113–120.\n",
      "\n",
      "[57] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A\n",
      "Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct:\n",
      "Aligning Language Model with Self Generated Instructions. arXiv\n",
      "preprint arXiv:2212.10560 (2022).\n",
      "\n",
      "[58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\n",
      "Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf,\n",
      "Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural\n",
      "language processing. In Proceedings of the 2020 conference on empirical\n",
      "methods in natural language processing: system demonstrations. 38–45.\n",
      "[59] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad\n",
      "Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\n",
      "Klaus Macherey, et al. 2016. Google’s neural machine translation\n",
      "system: Bridging the gap between human and machine translation.\n",
      "arXiv preprint arXiv:1609.08144 (2016).\n",
      "\n",
      "[60] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and\n",
      "Byung-Gon Chun. 2022. Orca: A Distributed Serving System for\n",
      "{Transformer-Based} Generative Models. In 16th USENIX Symposium\n",
      "on Operating Systems Design and Implementation (OSDI 22). 521–538.\n",
      "[61] Hong Zhang, Yupeng Tang, Anurag Khandelwal, and Ion Stoica. 2023.\n",
      "SHEPHERD: Serving DNNs in the Wild. In 20th USENIX Symposium on\n",
      "Networked Systems Design and Implementation (NSDI 23). USENIX As-\n",
      "sociation, Boston, MA, 787–808. https://www.usenix.org/conference/\n",
      "nsdi23/presentation/zhang-hong\n",
      "\n",
      "\f[62] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,\n",
      "Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria\n",
      "Lin, et al. 2022. Opt: Open pre-trained transformer language models.\n",
      "arXiv preprint arXiv:2205.01068 (2022).\n",
      "\n",
      "[63] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng\n",
      "Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo,\n",
      "\n",
      "Eric P Xing, et al. 2022. Alpa: Automating Inter-and Intra-Operator\n",
      "Parallelism for Distributed Deep Learning. In 16th USENIX Symposium\n",
      "on Operating Systems Design and Implementation (OSDI 22). 559–578.\n",
      "[64] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. 2022. PetS:\n",
      "A Unified Framework for Parameter-Efficient Transformers Serving. In\n",
      "2022 USENIX Annual Technical Conference (USENIX ATC 22). 489–504.\n",
      "\n",
      "16\n",
      "\n",
      " model output {\"references\": [\n",
      "        { \"title\": \"Paper Title 1\", \"authors\": [\"A\", \"B\"], \"date\": \"2023-01\",\n",
      "           \"journal\": \"J. XYZ\" },\n",
      "        { \"title\": \"Paper Title 2\", \"authors\": [\"C\", \"D\"],\n",
      "           \"date\": \"2023-02\", \"journal\": \"K. L.\"\n",
      "        }\n",
      "        ]\n",
      "    }\n",
      "titles_count 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prompts import *\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "articles_file = \"RE_articles.csv\"\n",
    "articles_df = pd.read_csv(articles_file, delimiter=\";\", header=None)\n",
    "articles_df.columns = ['patharticle']\n",
    "\n",
    "def extract_references_count(article_text, llm_name):\n",
    "    references_json = extract_references_with_prompts(article_text, llm_name)\n",
    "    \n",
    "    if isinstance(references_json, str):\n",
    "        try:\n",
    "            references_data = json.loads(references_json)\n",
    "            \n",
    "            if \"references\" in references_data and isinstance(references_data[\"references\"], list):\n",
    "                titles_count = sum(1 for ref in references_data[\"references\"] if \"title\" in ref)\n",
    "                print(\"titles_count\",titles_count)\n",
    "                return titles_count\n",
    "            else:\n",
    "                return 0\n",
    "        except json.JSONDecodeError:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "num_references_list = []\n",
    "\n",
    "for article_path in articles_df['patharticle']:\n",
    "    article_text = get_txt_content(article_path)\n",
    "    llm_name = \"deepseek-r1:1.5b\"\n",
    "    \n",
    "    num_references = extract_references_count(article_text, llm_name)\n",
    "    num_references_list.append(num_references)\n",
    "\n",
    "articles_df['num_references'] = num_references_list\n",
    "\n",
    "articles_df.to_csv(\"RE_ours_references.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Exactitude) du nombre de références extraites : 7.14%\n"
     ]
    }
   ],
   "source": [
    "ground_truth_file = \"RE_ground_truth_references.csv\"\n",
    "ground_truth = pd.read_csv(ground_truth_file, delimiter=\";\", header=None)\n",
    "ground_truth.columns = ['patharticle', 'num_references']  \n",
    "\n",
    "def evaluate_reference_count(extracted_counts, ground_truth_counts):\n",
    "    correct_count = 0\n",
    "    total_articles = len(extracted_counts)\n",
    "\n",
    "    for i in range(total_articles):\n",
    "        if extracted_counts[i] == ground_truth_counts[i]:\n",
    "            correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / total_articles\n",
    "    return accuracy\n",
    "\n",
    "ground_truth_counts = []\n",
    "extracted_counts = articles_df['num_references'].tolist()\n",
    "\n",
    "for article_path in articles_df['patharticle']:\n",
    "    ground_truth_article = ground_truth[ground_truth['patharticle'] == article_path]\n",
    "    ground_truth_count = ground_truth_article['num_references'].values[0]  # Nombre de références pour l'article\n",
    "    ground_truth_counts.append(ground_truth_count)\n",
    "\n",
    "accuracy = evaluate_reference_count(extracted_counts, ground_truth_counts)\n",
    "\n",
    "print(f\"Accuracy (Exactitude) du nombre de références extraites : {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT_References_Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_references = pd.read_csv(\"RE_ground_truth_references.csv\", header=None, names=[\"path_expected\", \"num_references_expected\"], delimiter=\";\")\n",
    "csv_observations = pd.read_csv(\"RE_o1_references.csv\", header=None, names=[\"path_observed\", \"num_references_observed\"], delimiter=\";\")\n",
    "\n",
    "merged = pd.merge(csv_references, csv_observations, left_on=\"path_expected\", right_on=\"path_observed\", suffixes=(\"_expected\", \"_observed\"))\n",
    "\n",
    "merged[\"match\"] = merged[\"num_references_expected\"] == merged[\"num_references_observed\"]\n",
    "\n",
    "match_count = merged[\"match\"].sum()\n",
    "\n",
    "accuracy = match_count / len(merged)\n",
    "\n",
    "# o1 results\n",
    "print(f\"GPT_References_Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
